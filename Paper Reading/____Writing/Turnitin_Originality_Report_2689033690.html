
    
<!DOCTYPE html>
<html lang="zh-hans">

<meta http-equiv="X-UA-Compatible" content="IE=7" />

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <meta name="author" content="Turnitin, LLC" />
    <meta name="keywords" content="" /> 
    <meta name="description" content="" />
<title>Turnitin - 原创性报告 - VRExplorer_A_Model_based_Approach_for_Automated_Virtual_Reality_Scene_Testing.pdf </title>

<base href="http://www.turnitin.com">
<style type="text/css">
body {
    color: #333;
    background: #C0C7CC;
    padding: 0;
    border: 0;
    font: 13px Verdana, arial, sans-serif;
    margin: 0;
}

form {
    padding: 0;
    margin: 0;
}

body#display {
}

body#bodysource {
    width: 520px;
    background: #F0F4FA;
}

p {
    padding: 10px 18px;
    margin: 0;
}

img {
    border: 0;
    padding: 0;
}

div {
    padding: 0;
    border: 0;
}

iframe {
    border: 0;
    margin: 0;
    padding: 0;
}

strong {
    font-weight: bold;
}

ul {
    padding: 0;
    margin: 0;
    list-style-type: none;
    font-size: 13px;
}

ul li {
    padding: 0;
    margin: 0;
    line-height: 16px;
}

#index span#exclude {
    margin: 0 50px 0 23px;
}

#index a {
    font-size: 11px;
    padding: 0 8px;
}

#index select {
    font-size: 12px;
    border: 1px solid #888;
}

#index input.small {
    margin: 0 0 0 5px;
    width: 30px;
    color: #D10A0A;
    font-weight: bold;
    font-size: 13px;
    border: 1px solid #888;
    vertical-align: baseline;
}

div.links {
    width: 85%;
    margin: 0 auto;
    border-left: 1px solid #888;
    border-right: 1px solid #888;
    padding-top: 8px;
    background: #E8EEF7;
    text-align: left;
}

.links div {
    padding: 5px 13px 10px 20px;
    border-bottom: 1px dotted #888;
}

.links div p {
    padding: 2px 0 0 40px;
}

div#body {
    line-height: 17px;
    width: 85%;
    margin: auto;
    padding: 20px 0;
    background: #fff;
    border-bottom: 1px solid #888;
    border-right: 1px solid #888;
    border-left: 1px solid #888;
    text-align: left;
}

#body p {
    color: #000;
    padding-top: 10 0;
    margin: 0 40px;
}

#actions {
    display: none;
}

a.exclude {
    float: right;
    margin: 0;
    padding: 0;
    position: relative;
    bottom: 20px;
}

/*= SMALL MATCHES POPUP
=== === === === === === === === === === === === === === === === === === === === === === === === === === === === === === */
div#small_matches_prefs {
    visibility: hidden;
    position: absolute;
    top: 0;
    left: 400px;
    background-color: #FFF;
    border: 1px solid #999;
    text-align: right;
}

div#small_matches_prefs p {
    padding: 7px;
}

div#small_matches_prefs li {
    padding: 10px 40px 10px 0;
    border-bottom: 1px solid #999;
    cursor: pointer;
    text-align: left;
}

div#small_matches_prefs li.selected {
    background-color: #87A3C0;
}

div#small_matches_prefs li input {
    text-align: center;
    border: 1px solid #999;
}

div#small_matches_prefs li.disabled input {
    color: #878787;
    background-color: #E6E5E6;
}

div#small_matches_prefs ul label {
    width: 100px;
    text-align: right;
    display: inline-block;
    margin-left: 10px;
    margin-right: 10px;
}
/*= GENERAL
=== === === === === === === === === === === === === === === === === === === === === === === === === === === === === === */

body #top_bar {
    display: none !important;
}

body #index #exclude,
#download_button,
#print_button,
#index .right {
    display: inline-block;
}

body #index {
    width: 85%;
    margin-left: auto;
    margin-right: auto;
    border: 1px solid #999;
    background: #ececec url(new_dynamic/images/22bd7a01a025b8de122259e42762f0a7cb_ug_toolbar_bg.gif) repeat-x center left;
}

#toolbar_wrapper {
    padding-left: 45px;
}

body #top {
    width: 85%;
    background-color: #FFF;
    margin-left: auto;
    margin-right: auto;
    border: 1px solid #999;
    border-bottom: none;
    height: 210px;
}

body #content {
    padding: 10px 60px;
}

body div#prefs {
    display: none;
}

body #top h1 {
    font-size: 20px;
    font-weight: normal;
}

body #top h1 strong {
    font-weight: normal;
}

body #top h1 em {
    font-style: normal;
}


/*body #top h2 {*/
/*    font-size: 20px;*/
/*    font-weight: normal;*/
/*}*/

/*body #top h2 strong {*/
/*    font-weight: normal;*/
/*}*/

/*body #top h2 em {*/
/*    font-style: normal;*/
/*}*/


body #top h2 {
    font-size: 16px;
    font-weight: normal;
}

body #top h2 strong {
    font-weight: normal;
}

body #top h2 em {
    font-style: normal;
}


body #top_body li { /*Paper info li*/
    padding: 0;
    margin: 0px 0px 2px 0px;
    font-size: 10px;
}

#top_body #print_wrapper {
    float: left;
    width: 50%;
}

#top_body .similarity_print_wrapper {
    width: 45%;
    min-width: 283px;
}

#top_body .similarity_box { /*Similarity Box w/ Similarity by Source */
    float: right;
    border: solid 1px #666;
    margin-top: 60px;
    min-width: 350px;
}

#top_body .similarity_box .overall_similarity {
    float: left;
    border-right: solid 1px #666;
}

#top_body .similarity_box .overall_similarity .color_box {
    font-size: 14px;
    min-width: 140px;
}

#top_body .color_box.green {
    background-color: green;
}

#top_body .color_box.blue {
    background-color: blue;
}

#top_body .color_box.yellow {
    background-color: yellow;
}

#top_body .color_box.orange {
    background-color: orange;
}

#top_body .color_box.red {
    background-color: red;
}

#top_body .similarity_box .overall_similarity .similarity_title {
    font-size: 13px;
    font-weight: normal;
    padding: 5px 5px 5px;
    text-align: center;
}

#top_body .similarity_box .overall_similarity .similarity_percent {
    font-size: 25px;
    font-family: georgia, times, serif;
    padding: 5px 0px 15px;
    text-align: center;
}

#top_body .similarity_box .overall_similarity a {
    display: none;
}

#top_body .similarity_box .similarity_by_source {
    float: right;
    font-size: 10px;
}

#top_body .similarity_box .similarity_by_source .similarity_title {
    padding: 6px 0px 0px 10px;
    font-weight: bold;
    text-align: left;
}

#top_body .similarity_box .similarity_by_source dl {
    padding-left: 10px;
    margin: 14px 7px 0px 0px;
}

#top_body .similarity_box .similarity_by_source dt {
    float: left;
    width: 160px;
}

#top_body .similarity_box .similarity_by_source dd {
    float: left;
    margin: 0px;
}


#index span#exclude {
    margin: 0 50px 0 23px;
}

#index a {
    font-size: 11px;
    padding: 0 8px;
}

#index select {
    font-size: 12px;
    border: 1px solid #888;
}

#index input.small {
    margin: 0 0 0 5px;
    width: 30px;
    color: #D10A0A;
    font-weight: bold;
    font-size: 13px;
    border: 1px solid #888;
    vertical-align: baseline;
}

div.links {
    width: 85%;
    margin: 0 auto;
    border-left: 1px solid #888;
    border-right: 1px solid #888;
    padding-top: 8px;
    background: #E8EEF7;
    text-align: left;
}

.links div {
    padding: 5px 13px 10px 20px;
    border-bottom: 1px dotted #888;
}

.links div p {
    padding: 2px 0 0 40px;
}

div#body {
    line-height: 17px;
    width: 85%;
    margin: auto;
    padding: 20px 0;
    background: #fff;
    border-bottom: 1px solid #888;
    border-right: 1px solid #888;
    border-left: 1px solid #888;
    text-align: left;
}

#body p {
    color: #000;
    padding-top: 10 0;
    margin: 0 40px;
}

#actions {
    display: none;
}

button.exclude {
    float: right;
    margin: 0;
    padding: 0;
    border: none;
}

#small_matches_prefs {
    display: none;
}

/*
Copyright (c) 2009, Yahoo! Inc. All rights reserved.
Code licensed under the BSD License:
http://developer.yahoo.net/yui/license.txt
version: 2.7.0
*/
.yui-button{display:-moz-inline-box;display:inline-block;vertical-align:text-bottom;}.yui-button .first-child{display:block;*display:inline-block;}.yui-button button,.yui-button a{display:block;*display:inline-block;border:none;margin:0;}.yui-button button{background-color:transparent;*overflow:visible;cursor:pointer;}.yui-button a{text-decoration:none;}.yui-skin-sam .yui-button{border-width:1px 0;border-style:solid;border-color:#808080;background:url(../images/yui270/build/assets/skins/sam/96b257a32a932f7739d7dab52b38ee8fcb_sprite.png) repeat-x 0 0;margin:auto .25em;}.yui-skin-sam .yui-button .first-child{border-width:0 1px;border-style:solid;border-color:#808080;margin:0 -1px;_margin:0;}.yui-skin-sam .yui-button button,.yui-skin-sam .yui-button a{padding:0 10px;font-size:93%;line-height:2;*line-height:1.7;min-height:2em;*min-height:auto;color:#000;}.yui-skin-sam .yui-button a{*line-height:1.875;*padding-bottom:1px;}.yui-skin-sam .yui-split-button button,.yui-skin-sam .yui-menu-button button{padding-right:20px;background-position:right center;background-repeat:no-repeat;}.yui-skin-sam .yui-menu-button button{background-image:url(yui270/build/button/assets/skins/sam/6305efb37fa05af65c79b58b9d4c1b03cb_menu-button-arrow.png);}.yui-skin-sam .yui-split-button button{background-image:url(yui270/build/button/assets/skins/sam/ced974d5c685e5dfa0a37b824a6b5d48cb_split-button-arrow.png);}.yui-skin-sam .yui-button-focus{border-color:#7D98B8;background-position:0 -1300px;}.yui-skin-sam .yui-button-focus .first-child{border-color:#7D98B8;}.yui-skin-sam .yui-button-focus button,.yui-skin-sam .yui-button-focus a{color:#000;}.yui-skin-sam .yui-split-button-focus button{background-image:url(yui270/build/button/assets/skins/sam/36e66540d2feba76b8991e18b76fe93bcb_split-button-arrow-focus.png);}.yui-skin-sam .yui-button-hover{border-color:#7D98B8;background-position:0 -1300px;}.yui-skin-sam .yui-button-hover .first-child{border-color:#7D98B8;}.yui-skin-sam .yui-button-hover button,.yui-skin-sam .yui-button-hover a{color:#000;}.yui-skin-sam .yui-split-button-hover button{background-image:url(yui270/build/button/assets/skins/sam/36e66540d2feba76b8991e18b76fe93bcb_split-button-arrow-hover.png);}.yui-skin-sam .yui-button-active{border-color:#7D98B8;background-position:0 -1700px;}.yui-skin-sam .yui-button-active .first-child{border-color:#7D98B8;}.yui-skin-sam .yui-button-active button,.yui-skin-sam .yui-button-active a{color:#000;}.yui-skin-sam .yui-split-button-activeoption{border-color:#808080;background-position:0 0;}.yui-skin-sam .yui-split-button-activeoption .first-child{border-color:#808080;}.yui-skin-sam .yui-split-button-activeoption button{background-image:url(yui270/build/button/assets/skins/sam/890272b241c1d8a0db3ce5680b71fab0cb_split-button-arrow-active.png);}.yui-skin-sam .yui-radio-button-checked,.yui-skin-sam .yui-checkbox-button-checked{border-color:#304369;background-position:0 -1400px;}.yui-skin-sam .yui-radio-button-checked .first-child,.yui-skin-sam .yui-checkbox-button-checked .first-child{border-color:#304369;}.yui-skin-sam .yui-radio-button-checked button,.yui-skin-sam .yui-checkbox-button-checked button{color:#fff;}.yui-skin-sam .yui-button-disabled{border-color:#ccc;background-position:0 -1500px;}.yui-skin-sam .yui-button-disabled .first-child{border-color:#ccc;}.yui-skin-sam .yui-button-disabled button,.yui-skin-sam .yui-button-disabled a{color:#A6A6A6;cursor:default;}.yui-skin-sam .yui-menu-button-disabled button{background-image:url(yui270/build/button/assets/skins/sam/4df7235ca027f2546b2a216e59f81fb0cb_menu-button-arrow-disabled.png);}.yui-skin-sam .yui-split-button-disabled button{background-image:url(yui270/build/button/assets/skins/sam/db73dce6da2f5c5f02399c93488ce69ecb_split-button-arrow-disabled.png);}

</style>



</head>

<body onload="">



<link rel="stylesheet" type="text/css" href="/r/build/css/tii/88ee4ccd3555f2b759921fb5d58d83e5cb_container.css" media="all" />




<script type="text/javascript" src="/r/build/js/tii/8b608684a5f4aec1b540987c93498c01cb_tii_anonymous_marking.js"></script>




<script type="text/javascript">

function initAnonymousMarking () {
    // initialize panel.  
    var config = {
            zindex: 4,
            underlay: 'none',
            modal: true,
            visible: false,
            draggable: false,
            close: false,
            fixedcenter: true
    };
    if ($('disable_anonymous_marking')) {
        disableAnonymousMarkingPanel = new IP.widget.Panel($('disable_anonymous_marking'), config);
        if($D.hasClass('disable_anonymous_marking', 'app')) {
            disableAnonymousMarkingPanel.center = function () {
                var nViewportOffset = 20,
                    elementWidth = this.element.offsetWidth,
                    elementHeight = this.element.offsetHeight,
                    viewPortWidth = $D.getViewportWidth(),
                    viewPortHeight = $D.getViewportHeight(),
                    x,
                    y;

                if (elementWidth < viewPortWidth) {
                    x = (viewPortWidth / 2) - (elementWidth / 2) + $D.getDocumentScrollLeft();
                } else {
                    x = nViewportOffset + $D.getDocumentScrollLeft();
                }

				if (browser == 'Internet Explorer') {
					x = 0;
				}
                y = 2 + $D.getDocumentScrollTop();

                this.cfg.setProperty("xy", [parseInt(x, 10), parseInt(y, 10)]);
                this.cfg.refireEvent("iframe");
            };
        }
        disableAnonymousMarkingPanel.render(document.body);
        disableAnonymousMarkingPanel.hideEvent.subscribe(function () { $('anonymous_error').innerHTML = ''; }, false);
        disableAnonymousMarkingPanel.hide();
        Element.show($('disable_anonymous_marking'));
    }
}

function disableAM (data) {
    $('anonymous_title').innerHTML = data.title;
    document.disable_anonymous_marking_form.objectid.value = data.oid;
    disableAnonymousMarkingPanel.show();
}

function checkDisableAM () {
    var form = document.disable_anonymous_marking_form;
    if (form.reason.value.length <= 5) {
        $('anonymous_error').innerHTML = "请提供关闭匿名标记的原因。您的理由必须超过 10 个字符长度。";
    }
    else {
        form.submit();
    }
    return;
}

YAHOO.util.Event.onDOMReady(initAnonymousMarking);

</script>

<div id="disable_anonymous_marking" class="app" style="display: none;">
<div class="anonymous_frames">
<form method="post" name="disable_anonymous_marking_form">
    <input type="hidden" name="objectid" value=""/>
    <input type="hidden" name="disable_anonymous_marking" value="1"/> 
    <div class="anonymous_header">
    	<h1>关闭匿名标记</h1>
		<p>请说明关闭匿名标记的原因： <span id="anonymous_title"></span><br />
			<strong>警告: 管理员可以存取此资讯。此设定是永久性的。</strong>
        
            </p>
        
    </div>
    <div class="anonymous_body">
        <textarea name="reason" cols="30" rows="3"></textarea>
        <p id="anonymous_error"></p>
    </div>
    <div class="anonymous_footer">
		<div class="anonymous_footer_buttons">
            <span class="submit_form_button"><input type="button" onClick="checkDisableAM();" value="提交"></span><br>
        	<span class="submit_form_button"><input type="button" value="取消" onClick="disableAnonymousMarkingPanel.hide();"></span>
		</div>
    </div>
</form>
</div>
</div>
<div id="actions">
<p>这是您报告的印表版的检视。请点击 "打印" 以继续或 "结束" 以关闭视窗。</p>
<script type="text/javascript" language="javascript">
	var browserName=navigator.appName;
	var browserVer=parseInt(navigator.appVersion);
	if ((navigator.appVersion.indexOf("Mac")!=-1) && (browserName == "Microsoft Internet Explorer")) {
		document.write('<span class="AR10">键入 COMMAND-P 开始打印。</span><br><br>');
	} else {
		document.write('<button onclick="window.print();">打印</button>&nbsp;&nbsp;');
	}
</script>
<button onclick="window.close();">完成</button>
</div>


<!-- ########################### Preferences pop-up ##########################--> 

<div name="top" id="header">

<div id="prefs" role="dialog" style="display:none" aria-labelledby="prefs_link" aria-describedby="prefs_link" aria-owns="prefs_link">
<div class="padding">
<form name="prefs_form" method="post" accept-charset="utf-8">
<script type="text/javascript" language="javascript">
function savePrefs(){
	if (document.prefs_form.changed.value == 1){
		document.prefs_form.submit();
	}else{
        hidePrefsPane();
	}
}

function handlePrefsPaneKeyUp (evt) {
    // first check for IME compositions and ignore
    if (evt.isComposing || evt.keyCode === 229) {
        return;
    }
    if (evt.key === "Escape") {
        evt.preventDefault();
        evt.stopPropagation();
        hidePrefsPane();
    }
}

function showPrefsPane(){
    const prefsDiv = document.getElementById('prefs');
    prefsDiv.style.display='block';
    prefsDiv.addEventListener('keyup', handlePrefsPaneKeyUp);
    document.getElementById('use_colors').focus();
}

function hidePrefsPane(){
    const prefsDiv = document.getElementById('prefs');
    prefsDiv.style.display='none';
    prefsDiv.removeEventListener('keyup', handlePrefsPaneKeyUp);
    document.getElementById('prefs_link').focus();
}

var overlay, $D, $;

function handleSmallMatchesPrefKeyUp (evt) {
    // first check for IME compositions and ignore
    if (evt.isComposing || evt.keyCode === 229) {
        return;
    }
    if (evt.key === "Escape") {
        evt.preventDefault();
        evt.stopPropagation();
        hideSmallMatchExclusions();
    }
}

function showSmallMatchExclusions(left) {
    $D = YAHOO.util.Dom;
    $E = YAHOO.util.Event;
    $ = $D.get;

    $D.setStyle('small_matches_prefs', 'top', $D.getDocumentScrollTop() + 187 + 'px');
    $D.setStyle('small_matches_prefs', 'left', left + 'px');

    $D.setStyle('small_matches_prefs', 'display', 'block');
    $D.setStyle('small_matches_prefs', 'visibility', 'visible');

        // focus the field
    $D.hasClass('exclude_by_percent_row', 'selected') ? $('exclude_by_percent_value').focus() : $('exclude_by_words_value').focus();
    $E.on(window, 'scroll', repositionDialog);



/*
A problem occurs: when the focus moves from the wordcount field to the percentage field
the existing percentage is floored. So even if the value is "correct", it gets incorrect,
because the floored percentage is different from the word count. Especially in bigger texts and smaller matches
this becomes an issue. So, the percentage displayed when updated from the word count should be the rounded one.
only when the input itself is given manually, should this override the exclusionPercent value.

This can be done as long as we use the functions below as actual keyboard handlers, so we can filter between numerical
values entered and other keys (such as tab). Moreover, we could do up and down to increase or decrease the value.

Because we will need to keep a state of the unrounded percentage, it is better to have that percentage value
closed over.

*/

    let rawPercentage = 0;
    function updateExcludePercentage(evt) {
        // prevent symbol composing to interfere, 229 is a special code for the composition key
        if (evt.isComposing || evt.keyCode === 229) {
            return;
        }
        if (evt.key === "Escape") {
            evt.preventDefault();
            evt.stopPropagation();
            hideSmallMatchExclusions();
        }
        else if (/[0-9]/.exec(evt.key) || evt.key === 'Backspace' || evt.key === 'Delete') { //only recalculate when the entered keys represent numbers
            // only when entering or changing the value in the input field, it is clear that the user
            // intended to use this specific exclusion method
            selectSmallExclusionMethod('words');
            const wordCount = this.value;

            rawPercentage = wordCount / 10244 * 100;
            $('exclude_by_percent_value').value = Math.floor(rawPercentage); // only display floored value
        }
    }

    function updateExcludeWordCount(evt) {
        // prevent symbol composing to interfere, 229 is a special code for the composition key
        if (evt.isComposing || evt.keyCode === 229) {
            return;
        }
        if (evt.key === "Escape") {
            evt.preventDefault();
            evt.stopPropagation();
            hideSmallMatchExclusions();
        }
        else if (/[0-9]/.exec(evt.key) || evt.key === 'Backspace' || evt.key === 'Delete') { //only recalculate when the entered keys represent numbers
            // only when entering or changing the value in the input field, it is clear that the user
            // intended to use this specific exclusion method
            selectSmallExclusionMethod('percent');

            const percent = this.value;
            var wordCount = Math.floor((percent/100) * 10244);

            $('exclude_by_words_value').value = wordCount;
        }
    }
    // set to global name space so hideSmallMatchesExclusions can also remove the listeners.
    if (!window.updateExcludePercentage) window.updateExcludePercentage = updateExcludePercentage;
    if (!window.updateExcludeWordCount) window.updateExcludeWordCount = updateExcludeWordCount;

    document.getElementById('small_matches_prefs').addEventListener('keyup', handleSmallMatchesPrefKeyUp);
    document.getElementById('exclude_by_words_value').addEventListener('keyup', updateExcludePercentage);
    document.getElementById('exclude_by_percent_value').addEventListener('keyup', updateExcludeWordCount);

}

function repositionDialog() {
    $D.setStyle('small_matches_prefs', 'top', $D.getDocumentScrollTop() + 187 + 'px');
}

function hideSmallMatchExclusions() {
    document.getElementById('small_matches_prefs').removeEventListener('keyup', handleSmallMatchesPrefKeyUp);
    document.getElementById('exclude_by_words_value').removeEventListener('keyup', updateExcludePercentage);
    document.getElementById('exclude_by_percent_value').removeEventListener('keyup', updateExcludeWordCount);

    $D.setStyle('small_matches_prefs', 'display', 'none');
    $E.removeListener(window, 'scroll', repositionDialog);
    document.getElementById('exclude_small_matches_link').focus();
}

function selectSmallExclusionMethod(enableType) {
    if(enableType == 'words') {
        $('exclude_by_words_value').focus();
        
        $D.addClass('exclude_by_words_row', 'selected');
        $D.removeClass('exclude_by_percent_row', 'selected');
        $D.removeClass('exclude_by_words_row', 'disabled');
        $D.addClass('exclude_by_percent_row', 'disabled');
    }
    else {
        $('exclude_by_percent_value').focus();
        
        $D.removeClass('exclude_by_words_row', 'selected');
        $D.addClass('exclude_by_percent_row', 'selected');
        $D.addClass('exclude_by_words_row', 'disabled');
        $D.removeClass('exclude_by_percent_row', 'disabled');
    }
}

function submitSmallMatchesChange() {
    var excludeBy = $D.hasClass('exclude_by_percent_row', 'selected') ? 'percent' : 'words';
    var excludeValue = excludeBy == 'percent' ? $('exclude_by_percent_value').value : $('exclude_by_words_value').value;
    
    changeSmallMatchExclusion(excludeBy, parseInt(excludeValue), 10244);
}


</script>
<input type="hidden" name="changed" value="0">
        <div class="pref_rows">
                <label for="use_colors">颜色代码匹配：</label>
                <select id="use_colors" name="use_colors" onchange="document.prefs_form.changed.value=1">
                    <option value="1">是
                    <option value="0">否 
                </select>
                <div class="clear"></div>
        </div>
        <div class="pref_rows">
                <label for="def_report_mode">预设模式：</label>
                <select id="def_report_mode" name="def_report_mode" onchange="document.prefs_form.changed.value=1">
                    <option value="0">显示所有匹配度最高的
                    <option value="1">每次显示一个匹配的
                    <option value="2">快速查看报告（经典页面）
                </select>
                <div class="clear"></div>
        </div>
        <div class="pref_rows">
                <label for="report_scrolling">自动导航</label>
                <select id="report_scrolling" name="report_scrolling" onchange="document.prefs_form.changed.value=1">
                    <option value="0">跳到下一个符合处
                    <option value="1">移动到下一个谋和处
                </select>
                <div class="clear"></div>
        </div>
        
        <div id="prefs_confirm">
            <button onClick="savePrefs()" >储存</button>  
            <button onClick="hidePrefsPane();">取消</button>
        </div>
</form>
</div>
</div>
</div>
<!-- ########################### END Preferences pop-up  ##########################--> 

<!-- ########################### BEGIN small matches pop-up  ##########################--> 
<div id="small_matches_prefs" role="dialog" aria-labelledby="exclude_small_matches_link" aria-describedby="exclude_small_matches_link" aria-owns="exclude_small_matches_link">
    <form onsubmit="submitSmallMatchesChange(); return false;">
        <ul>
            <li id="exclude_by_words_row" class="selected">
                <label for="exclude_by_words_value">字数: </label>
                <input type="text" id="exclude_by_words_value" size="3" value="" onkeyup="updateExcludePercentage"> 字
            </li>
            <li id="exclude_by_percent_row" class="disabled">
                <label for="exclude_by_percent_value">百分比: </label>
                <input type="text" id="exclude_by_percent_value" size="3" value="" max-length="3" onkeyup="updateExcludeWordCount"> %
            </li>
        </ul>
        <p><input type='submit' value="提交"> 或 <button onclick="hideSmallMatchExclusions()">取消</button></p>
    </form>
</div>

<!-- ########################### END small matches pop-up  ##########################-->

<!-- ########################### Top of Report  ##########################--> 
<div id="top">
    <div id="content" role="banner">
    
        <!-- ######### Top Bar  ##########################--> 
        <div id="top_bar">
                <ul id="top_bar_list1">
                      <!-- Preferences --><li><button id="prefs_link" onclick="showPrefsPane(); aria-haspopup="dialog">preferences</button></li>
                </ul>
                <ul id="top_bar_list2">
                      
                </ul>
                <div class="clear"></div>
        </div>   
        <!-- ######### END Top Bar  ##########################--> 
        
        
        <!-- ######### Top Body  ##########################--> 
        <div id="top_body">
        
            <div id="print_wrapper">
                <div class="general_info" role="region" aria-label="文稿资讯">
                    <!-- Logo --> 
                    <h1>
                        <span class=""></span>
                        <strong>Turnitin</strong>
                        <em>原创性报告</em>
                     </h1>
                 
                     <!-- Paper Info -->               
                     <ul>
                         <li>已处理到: 2025年05月31日 19:54 CST</li>
                         <li>代码: 2689033690 </li>
                         <li>字数: 10244</li>
                         <li>已提交: 1</li>
                     </ul>
                </div>

                 <!-- Paper Title --> 
                <h2>
                    <strong>VRExplorer_A_Model_based_Approach_for_Automated_Virtual_Reality_Scene_Testing.pdf</strong> 
                    
                    <em>整合者 Check11399 .</em>
                    
                </h2>
            </div>
            
            <div id="similarity_print_wrapper">
                <div class="similarity_box" role="region" aria-labelledby="similarity_index_title" aria-describedby="similarity_index_title">
                    <div class="overall_similarity">
                        <div class="color_box green">&nbsp;</div>
                        <div id="similarity_index_title" class="similarity_title">相似度指标</div>
                        <div class="similarity_percent">4%</div>
                    </div>
                    <div class="similarity_by_source" role="region" aria-labelledby="similarity_by_source_title" aria-describedby="similarity_by_source_title">
                        <div id="similarity_by_source_title" class="similarity_title">依來源标示相似度</div>
                        <dl>
                            <dt>Internet&nbsp;Sources:</dt>
                            <dd>3%</dd>
                            <div class="clear"></div>
                            <dt>出版物:</dt>
                            <dd>3%</dd>
                            <div class="clear"></div>
                            <dt>学生文稿:</dt>
                            <dd>0%</dd>
                            <div class="clear"></div>
                        </dl>
                    </div>
                </div>
            </div>
            <div class="clear"></div>
                                 
        </div>
        <!-- ######### END Top Body  ##########################--> 
        
        
    </div>
</div>
<!-- ########################### END Top of Report  ##########################--> 



<!-- ########################### TOOLBAR  ##########################--> 
<div id="index">
	<div id="toolbar_wrapper" role="toolbar">
        
	</div>
</div>
<!-- ########################### END TOOLBAR  ##########################--> 


<div class="links" role="region" aria-label="相符总览">
    <div role="list">
	<div role="listitem" aria-setsize="34" aria-posinset="1">
	    <p>
	        < 1% match (从 2024年02月23日 的网络)
	    </p>
        
        
 
	    <p><a href="https://arxiv.org/html/2402.13815v1" target="_blank" style="color:#D10A0A">https://arxiv.org/html/2402.13815v1</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="2">
	    <p>
	        < 1% match (从 2022年01月21日 的网络)
	    </p>
        
        
 
	    <p><a href="https://arxiv.org/pdf/2201.06865.pdf" target="_blank" style="color:#287B28">https://arxiv.org/pdf/2201.06865.pdf</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="3">
	    <p>
	        < 1% match (从 2024年03月21日 的网络)
	    </p>
        
        
 
	    <p><a href="https://arxiv.org/html/2307.07221v2" target="_blank" style="color:blue">https://arxiv.org/html/2307.07221v2</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="4">
	    <p>
	        < 1% match ()
	    </p>
        
        
 
	    <p><a href="http://arxiv.org/abs/1907.09230" target="_blank" style="color:brown">Bardakov, Valeriy, Nasybullov, Timur. "Multi-switches and representations of braid groups", 2019</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="5">
	    <p>
	        < 1% match (从 2024年08月20日 的网络)
	    </p>
        
        
 
	    <p><a href="http://arxiv.org/pdf/2408.08515" target="_blank" style="color:#B64B01">http://arxiv.org/pdf/2408.08515</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="6">
	    <p>
	        < 1% match (从 2024年02月24日 的网络)
	    </p>
        
        
 
	    <p><a href="https://arxiv.org/html/2402.14096v1" target="_blank" style="color:#630000">https://arxiv.org/html/2402.14096v1</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="7">
	    <p>
	        < 1% match (从 2024年07月06日 的网络)
	    </p>
        
        
 
	    <p><a href="https://arxiv.org/html/2407.03037v1" target="_blank" style="color:#0270B6">https://arxiv.org/html/2407.03037v1</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="8">
	    <p>
	        < 1% match (Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, Zhendong Su. "Practical GUI Testing of Android Applications Via Model Abstraction and Refinement", 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 2019)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(642079831,37,'0')" target="_blank" style="color:#330099">Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, Zhendong Su. "Practical GUI Testing of Android Applications Via Model Abstraction and Refinement", 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 2019</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="9">
	    <p>
	        < 1% match (Xiaoyin Wang. "VRTest: An Extensible Framework for Automatic Testing of Virtual Reality Scenes", 2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(769448606,37,'0')" target="_blank" style="color:#227967">Xiaoyin Wang. "VRTest: An Extensible Framework for Automatic Testing of Virtual Reality Scenes", 2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="10">
	    <p>
	        < 1% match (Xiaoyin Wang, Tahmid Rafi, Na Meng. "VRGuide: Efficient Testing of Virtual Reality Scenes via Dynamic Cut Coverage", 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE), 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(808707852,37,'0')" target="_blank" style="color:#CB0099">Xiaoyin Wang, Tahmid Rafi, Na Meng. "VRGuide: Efficient Testing of Virtual Reality Scenes via Dynamic Cut Coverage", 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE), 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="11">
	    <p>
	        < 1% match (从 2022年10月01日 的网络)
	    </p>
        
        
 
	    <p><a href="https://web.archive.org/web/20220823101309if_/https:/arxiv.org/pdf/2208.07811v2.pdf" target="_blank" style="color:#006331">https://web.archive.org/web/20220823101309if_/https:/arxiv.org/pdf/2208.07811v2.pdf</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="12">
	    <p>
	        < 1% match (从 2023年11月23日 的网络)
	    </p>
        
        
 
	    <p><a href="https://export.arxiv.org/pdf/2302.09116" target="_blank" style="color:#795AB9">https://export.arxiv.org/pdf/2302.09116</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="13">
	    <p>
	        < 1% match (从 2023年11月27日 的网络)
	    </p>
        
        
 
	    <p><a href="http://export.arxiv.org/pdf/2308.06783" target="_blank" style="color:#935F32">http://export.arxiv.org/pdf/2308.06783</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="14">
	    <p>
	        < 1% match (从 2022年10月11日 来的学生文稿)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(707547647,2,'0')" target="_blank" style="color:#ce0031">Submitted to University of Bristol  on 2022-10-11</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="15">
	    <p>
	        < 1% match (Jian Hu. "CG-FL: A data augmentation approach using context-aware genetic algorithm for fault localization", Journal of Systems and Software, 2025)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(836534496,37,'0')" target="_blank" style="color:#866712">Jian Hu. "CG-FL: A data augmentation approach using context-aware genetic algorithm for fault localization", Journal of Systems and Software, 2025</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="16">
	    <p>
	        < 1% match (Xiongfei Wu, Jiaming Ye, Ke Chen, Xiaofei Xie, Yujing Hu, Ruochen Huang, Lei Ma, Jianjun Zhao. "Widget Detection-based Testing for Industrial Mobile Games", 2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(800126639,37,'0')" target="_blank" style="color:#63009c">Xiongfei Wu, Jiaming Ye, Ke Chen, Xiaofei Xie, Yujing Hu, Ruochen Huang, Lei Ma, Jianjun Zhao. "Widget Detection-based Testing for Industrial Mobile Games", 2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="17">
	    <p>
	        < 1% match (Oluwadamilare Harazeem Abdulganiyu, Taha Ait Tchakoucht, Ahmed El Hilali Alaoui, Yakub Kayode Saheed. "Attention-Driven Multi-Model Architecture for Unbalanced Network Traffic Intrusion Detection via Extreme Gradient Boosting", Intelligent Systems with Applications, 2025)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(836040931,37,'0')" target="_blank" style="color:#A85503">Oluwadamilare Harazeem Abdulganiyu, Taha Ait Tchakoucht, Ahmed El Hilali Alaoui, Yakub Kayode Saheed. "Attention-Driven Multi-Model Architecture for Unbalanced Network Traffic Intrusion Detection via Extreme Gradient Boosting", Intelligent Systems with Applications, 2025</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="18">
	    <p>
	        < 1% match (Yuxuan Li, Ruitao Feng, Sen Chen, Qianyu Guo, Lingling Fan, Xiaohong Li. "IconChecker: Anomaly Detection of Icon-Behaviors for Android Apps", 2021 28th Asia-Pacific Software Engineering Conference (APSEC), 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(769151950,37,'0')" target="_blank" style="color:#cc0066">Yuxuan Li, Ruitao Feng, Sen Chen, Qianyu Guo, Lingling Fan, Xiaohong Li. "IconChecker: Anomaly Detection of Icon-Behaviors for Android Apps", 2021 28th Asia-Pacific Software Engineering Conference (APSEC), 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="19">
	    <p>
	        < 1% match (从 2023年03月05日 的网络)
	    </p>
        
        
 
	    <p><a href="https://ar5iv.labs.arxiv.org/html/2103.06018" target="_blank" style="color:#21785B">https://ar5iv.labs.arxiv.org/html/2103.06018</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="20">
	    <p>
	        < 1% match (从 2025年01月17日 的网络)
	    </p>
        
        
 
	    <p><a href="https://ar5iv.org/html/2301.12453" target="_blank" style="color:#336699">https://ar5iv.org/html/2301.12453</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="21">
	    <p>
	        < 1% match (从 2025年03月11日 的网络)
	    </p>
        
        
 
	    <p><a href="https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2025.1550407/pdf" target="_blank" style="color:#D10A0A">https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2025.1550407/pdf</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="22">
	    <p>
	        < 1% match (Dhia Elhaq Rzig, Nafees Iqbal, Isabella Attisano, Xue Qin, Foyzul Hassan. "Virtual Reality (VR) Automated Testing in the Wild: A Case Study on Unity-Based VR Applications", Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(800204451,37,'0')" target="_blank" style="color:#287B28">Dhia Elhaq Rzig, Nafees Iqbal, Isabella Attisano, Xue Qin, Foyzul Hassan. "Virtual Reality (VR) Automated Testing in the Wild: A Case Study on Unity-Based VR Applications", Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="23">
	    <p>
	        < 1% match (Finlay Macklon, Mohammad Reza Taesiri, Markos Viggiato, Stefan Antoszko, Natalia Romanova, Dale Paas, Cor-Paul Bezemer. "Automatically Detecting Visual Bugs in HTML5 Canvas Games", Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, 2022)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(805781792,37,'0')" target="_blank" style="color:blue">Finlay Macklon, Mohammad Reza Taesiri, Markos Viggiato, Stefan Antoszko, Natalia Romanova, Dale Paas, Cor-Paul Bezemer. "Automatically Detecting Visual Bugs in HTML5 Canvas Games", Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, 2022</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="24">
	    <p>
	        < 1% match (Jiayuan Liang, Sinan Wang, Xiangbo Deng, Yepang Liu. "RIDA: Cross-App Record and Replay for Android", 2023 IEEE Conference on Software Testing, Verification and Validation (ICST), 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(797660729,37,'0')" target="_blank" style="color:brown">Jiayuan Liang, Sinan Wang, Xiangbo Deng, Yepang Liu. "RIDA: Cross-App Record and Replay for Android", 2023 IEEE Conference on Software Testing, Verification and Validation (ICST), 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="25">
	    <p>
	        < 1% match (Xinglong Yin, Mengxi Zhang, Tengmei Wang, Huaxiao Liu. "Efficient exploration of hard-to-find function in GUIs: A method and best practice", Displays, 2025)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(835932327,37,'0')" target="_blank" style="color:#B64B01">Xinglong Yin, Mengxi Zhang, Tengmei Wang, Huaxiao Liu. "Efficient exploration of hard-to-find function in GUIs: A method and best practice", Displays, 2025</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="26">
	    <p>
	        < 1% match ()
	    </p>
        
        
 
	    <p><a href="https://digitalcommons.kennesaw.edu/cs_etd/32" target="_blank" style="color:#630000">Karasek, Daniel. "SuperB: Superior Behavior-based Anomaly Detection Defining Authorized Users\u27 Traffic Patterns", DigitalCommons@Kennesaw State University, 2020</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="27">
	    <p>
	        < 1% match (从 2022年11月08日 的网络)
	    </p>
        
        
 
	    <p><a href="https://profs.scienze.univr.it/~ceccato/papers/2021/scam2021.pdf" target="_blank" style="color:#0270B6">https://profs.scienze.univr.it/~ceccato/papers/2021/scam2021.pdf</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="28">
	    <p>
	        < 1% match ()
	    </p>
        
        
 
	    <p><a href="https://rhul.elsevierpure.com/en/publications/0ef03bc6-b496-40af-b85e-c73da1ffd1c3" target="_blank" style="color:#330099">Wilkins, Ben. "Learning to Identify Bugs in Video Games", 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="29">
	    <p>
	        < 1% match (从 2025年03月14日 的网络)
	    </p>
        
        
 
	    <p><a href="https://zhendong2050.github.io/res/time-travel-testing-21-01-2020.pdf" target="_blank" style="color:#227967">https://zhendong2050.github.io/res/time-travel-testing-21-01-2020.pdf</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="30">
	    <p>
	        < 1% match (Álvaro Herrero, Cristian I. Pinzón, Emilio Corchado, Javier Bajo. "Chapter 84 Unsupervised Visualization of SQL Attacks by Means of the SCMAS Architecture", Springer Science and Business Media LLC, 2010)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(29014317,37,'0')" target="_blank" style="color:#CB0099">Álvaro Herrero, Cristian I. Pinzón, Emilio Corchado, Javier Bajo. "Chapter 84 Unsupervised Visualization of SQL Attacks by Means of the SCMAS Architecture", Springer Science and Business Media LLC, 2010</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="31">
	    <p>
	        < 1% match (Bo Jiang, Wenlin Wei, Li Yi, W.K. Chan. "DroidGamer: Android Game Testing with Operable Widget Recognition by Deep Learning*", 2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS), 2021)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(757568641,37,'0')" target="_blank" style="color:#006331">Bo Jiang, Wenlin Wei, Li Yi, W.K. Chan. "DroidGamer: Android Game Testing with Operable Widget Recognition by Deep Learning*", 2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS), 2021</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="32">
	    <p>
	        < 1% match (Liming Nie, Kabir Sulaiman Said, Lingfei Ma, Yaowen Zheng, Yangyang Zhao. "A systematic mapping study for graphical user interface testing on mobile apps", IET Software, 2023)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(798920533,37,'0')" target="_blank" style="color:#795AB9">Liming Nie, Kabir Sulaiman Said, Lingfei Ma, Yaowen Zheng, Yangyang Zhao. "A systematic mapping study for graphical user interface testing on mobile apps", IET Software, 2023</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="33">
	    <p>
	        < 1% match (Habib Ullah Khan, Yasir Ali, Faheem Khan, Mugahed A. Al-antari. "A comprehensive study on unraveling the advances of immersive technologies (VR/AR/MR/XR) in the healthcare sector during the COVID-19: Challenges and solutions", Heliyon, 2024)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(824769792,37,'0')" target="_blank" style="color:#935F32">Habib Ullah Khan, Yasir Ali, Faheem Khan, Mugahed A. Al-antari. "A comprehensive study on unraveling the advances of immersive technologies (VR/AR/MR/XR) in the healthcare sector during the COVID-19: Challenges and solutions", Heliyon, 2024</a>

    
    </p>
    </div>
	<div role="listitem" aria-setsize="34" aria-posinset="34">
	    <p>
	        < 1% match (Joakim Bergdahl, Camilo Gordillo, Konrad Tollmar, Linus Gisslen. "Augmenting Automated Game Testing with Deep Reinforcement Learning", 2020 IEEE Conference on Games (CoG), 2020)
	    </p>
        
        
 
	    <p><a href="javascript:openDSC(770706301,37,'0')" target="_blank" style="color:#ce0031">Joakim Bergdahl, Camilo Gordillo, Konrad Tollmar, Linus Gisslen. "Augmenting Automated Game Testing with Deep Reinforcement Learning", 2020 IEEE Conference on Games (CoG), 2020</a>

    
    </p>
    </div></div></div></div><div id="body" tabIndex="0" role="main"><p>VRExplorer: <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 11 in source list: https://web.archive.org/web/20220823101309if_/https:/arxiv.org/pdf/2208.07811v2.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=1115539198&n=3799&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">A Model-based Approach for Automated Testing of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Virtual Reality Scenes Anonymous Authors Abstract—With the proliferation of Virtual Reality (VR) markets, VR applications are rapidly expanding in scale and complexity, thereby driving an urgent need for assuring VR software quality. Different from traditional mobile applications and computer software, VR testing faces unique challenges due to diverse interactions with virtual objects, complex 3D virtual environments, and intricate sequences to complete tasks. All of these emerging challenges hinder existing VR testing tools from effectively and systematically testing VR applications. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: Xiaoyin Wang. "VRTest: An Extensible Framework for Automatic Testing of Virtual Reality Scenes", 2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE-Companion55297.2022.9793753', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">In this paper, we present</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> VRExplorer, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: Xiaoyin Wang. "VRTest: An Extensible Framework for Automatic Testing of Virtual Reality Scenes", 2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE-Companion55297.2022.9793753', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">a novel</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> model-based <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: Xiaoyin Wang. "VRTest: An Extensible Framework for Automatic Testing of Virtual Reality Scenes", 2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE-Companion55297.2022.9793753', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">testing</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> tool <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: Xiaoyin Wang. "VRTest: An Extensible Framework for Automatic Testing of Virtual Reality Scenes", 2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE-Companion55297.2022.9793753', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">to</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> effectively interact with diverse virtual objects and explore complex VR scenes. Particularly, we design the Entity, Action, and Task (EAT) framework for modeling diverse VR interactions in a generic way. Built upon the EAT framework, we then present the VRExplorer agent, which can achieve effective scene exploration by incorporating meticulously designed path-finding algorithms into Unity’s NavMesh. Moreover, the VRExplorer agent can also systematically execute interaction decisions on top of the Probabilistic Finite State Machine (PFSM). Experi- mental evaluation on nine representative VR projects shows that VRExplorer consistently outperforms the state-of-the-art (SOTA) approach VRGuide by achieving significantly higher coverage and better efficiency. Specifically, VRExplorer yields up to 72.8% and 46.9% improvements over VRGuide in terms of executable lines of code (ELOC) coverage and method (function) coverage, re- spectively. Furthermore, ablation results also verify the essential contributions of each designed module. More importantly, our VRExplorer has successfully detected two functional bugs and one non-functional bug from real-world projects. I. INTRODUCTION Virtual reality (VR), working together with other relevant technologies, such as <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 33 in source list: Habib Ullah Khan, Yasir Ali, Faheem Khan, Mugahed A. Al-antari. "A comprehensive study on unraveling the advances of immersive technologies (VR/AR/MR/XR) in the healthcare sector during the COVID-19: Challenges and solutions", Heliyon, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.heliyon.2024.e35037', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">Augmented Reality (AR</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">) and <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 33 in source list: Habib Ullah Khan, Yasir Ali, Faheem Khan, Mugahed A. Al-antari. "A comprehensive study on unraveling the advances of immersive technologies (VR/AR/MR/XR) in the healthcare sector during the COVID-19: Challenges and solutions", Heliyon, 2024"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.heliyon.2024.e35037', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">eXtended Reality (XR</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">), aims to provide users with an immersive mixed reality (MR) experience [1]. Current VR applications have proliferated in diverse fields, such as medical treatment, ed- ucation, audiovisual entertainment, training simulations, man- ufacturing, and gaming [1–6]. According to Fortune’s market report [7], <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://arxiv.org/html/2402.13815v1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=3972278150&n=3806&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">the global VR market is projected to grow from</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> $44.4 <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://arxiv.org/html/2402.13815v1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=3972278150&n=3806&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">billion in</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 2025 <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://arxiv.org/html/2402.13815v1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=3972278150&n=3806&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">to</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> $244.84 <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://arxiv.org/html/2402.13815v1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=3972278150&n=3806&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">billion</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> in 2032. With <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://arxiv.org/html/2402.13815v1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=3972278150&n=3806&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">the proliferation of VR</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> markets, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://arxiv.org/html/2402.13815v1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=3972278150&n=3806&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">VR</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> applications are <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 1 in source list: https://arxiv.org/html/2402.13815v1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=3972278150&n=3806&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">also</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> rapidly expanding in terms of scale and complexity. How to guarantee the software quality of such complex VR applications becomes an urgent need. As a critical procedure of software development, testing can thoroughly evaluate a software to check whether both requirements and functional needs are fulfilled without defects. To cater for this grow- ing demand in VR applications, extensive efforts have been made in VR testing, while many of them still largely rely on manual testing, which remains highly labor-intensive and inefficient [8, 9]. Most recently, several studies have aimed to test VR applications automatically. As an early attempt, VRTest [10] explores VR scenes by controlling the orientation of the camera to interact with virtual objects. VRGuide [11] has further improved VRTest by optimizing exploration routes to circumvent occluded objects. Besides VRTest and VRGuide, other researchers also explore using other techniques, such as computer vision (CV) and generative artificial intelligence (GenAI) to achieve scene exploration [12] or understand- ing [13]. However, these testing tools are still struggling to test VR applications with increased complexity. The challenges in VR testing stem from the unique fea- tures of VR applications, sharply different from conventional mobile applications and computer software. First, enabled by diverse peripheral devices (e.g., VR headsets, controllers, joysticks, wands, and haptic gloves), VR systems can support a larger diversity of interactions (such as grabbing, pressing, touching, pulling, climbing, and shooting) than conventional mobile systems and PCs. Unfortunately, current VR testing tools cannot properly characterize and represent the diverse interaction behaviors. For example, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 12 in source list: https://export.arxiv.org/pdf/2302.09116"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=2624083896&n=3805&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">the state-of-the-art</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> (SOTA) VR <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 12 in source list: https://export.arxiv.org/pdf/2302.09116"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=2624083896&n=3805&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">testing tool</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, VRGuide, can <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 12 in source list: https://export.arxiv.org/pdf/2302.09116"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=2624083896&n=3805&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">only</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> test virtual objects with the “click” interaction. Second, VR applications contain complex 3D virtual environments with interactions with virtual objects, thereby introducing a vast exploration state space. Take Es- capeGameVR, a popular open-source VR gaming application, as an example, which contains 44 scenes, 8,256 GameObjects, and 1,377 C# script files. Third, VR testing often requires com- pleting a sequence of tasks, e.g., finding a key, then using the key to open a door, next turning a handle, and finally pressing a button to escape. It is non-trivial to accomplish these complex tasks since they involve complex interactions and trigger either events or actions in a specific order. In summary, the diverse and intricate nature of the VR interactions, coupled with large- scale virtual spaces and complex task-completion sequences, makes it difficult to comprehensively and efficiently test VR applications in a systematic and repeatable manner. Our Approach. To address the above challenges, we present VRExplorer to thoroughly test Unity-based VR appli- cations by conducting in-depth scene exploration and compre- hensive interactions with virtual objects. Notably, we consider Unity-based VR applications mainly due to Unity’s dominant role in VR/MR markets [14]. To tackle the first challenge mentioned above, we design the Entity, Action, and Task (EAT) framework for modeling VR interaction behaviors in a generic way. This hierarchical framework enables a reusable modeling process across VR applications developed by diverse Unity versions and interaction plugins, such as XRIT [15, 16], STEAMVR [17–19], and MRTK [20, 21], thereby enhancing cross-project generalizability. To address the second chal- lenge, we present the VRExplorer agent built upon the EAT framework. Incorporating meticulously designed path-finding algorithms into Unity’s NavMesh, the VRExplorer agent can achieve autonomous navigation in 3D virtual environments. Moreover, the VRExplorer agent can also systematically exe- cute diverse interaction decisions on top of the Probabilistic Finite State Machine (PFSM). To address the third challenge, the proposed model-based approach transforms intricate VR task execution sequences into structured task models to enable the organized systematic testing and automated execution. Evaluation. To comprehensively evaluate the proposed VR- Explorer, we conduct extensive experiments on nine repre- sentative VR projects. The experimental results demonstrate that the proposed VRExplorer outperforms the SOTA approach VRGuide with average performance gains of 72.8% and 46.9% in terms of executable lines of code (ELOC) coverage and method (function) coverage, respectively. Moreover, the ab- lation study on the five ablated variants of VRExplorer also validates the necessity of all modules of the EAT framework. More importantly, VRExplorer has successfully detected two previously unknown real-world bugs (i.e., one functional bug and one non-functional bug), which can nevertheless be de- tected by the baseline approach. Further, VRExplorer has also successfully detected one previously-confirmed bug. Main Contributions. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 18 in source list: Yuxuan Li, Ruitao Feng, Sen Chen, Qianyu Guo, Lingling Fan, Xiaohong Li. "IconChecker: Anomaly Detection of Icon-Behaviors for Android Apps", 2021 28th Asia-Pacific Software Engineering Conference (APSEC), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/APSEC53868.2021.00028', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#cc0066" class="#cc0066">In summary, we make the following main</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> research <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 18 in source list: Yuxuan Li, Ruitao Feng, Sen Chen, Qianyu Guo, Lingling Fan, Xiaohong Li. "IconChecker: Anomaly Detection of Icon-Behaviors for Android Apps", 2021 28th Asia-Pacific Software Engineering Conference (APSEC), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/APSEC53868.2021.00028', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#cc0066" class="#cc0066">contributions: • We design</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the EAT framework for modeling complex interaction behaviors and tasks in VR <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 19 in source list: https://ar5iv.labs.arxiv.org/html/2103.06018"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=3429287884&n=3800&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">applications. To the best of our knowledge</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, EAT <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 19 in source list: https://ar5iv.labs.arxiv.org/html/2103.06018"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=3429287884&n=3800&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#21785B" class="#21785B">is the first</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> generic three- layer abstraction framework for VR testing based on object- oriented programming (OOP). • We present VRExplorer1, a novel model-based testing tool to achieve effective interactions with diverse virtual objects and the exploration of complex VR scenes. • We evaluate VRExplorer extensively on nine representative VR projects and demonstrate that it consistently outper- forms the SOTA approach <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, Zhendong Su. "Practical GUI Testing of Android Applications Via Model Abstraction and Refinement", 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE.2019.00042', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">in terms of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> ELOC <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, Zhendong Su. "Practical GUI Testing of Android Applications Via Model Abstraction and Refinement", 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE.2019.00042', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">coverage, method coverage, and</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> interactable object <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, Zhendong Su. "Practical GUI Testing of Android Applications Via Model Abstraction and Refinement", 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE.2019.00042', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">coverage</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> while preserving high efficiency. Moreover, our VRExplorer can also successfully detect real-world bugs in VR projects. II. BACKGROUND Interactions in Unity-based VR Applications. A Unity- based VR application typically consists of multiple Scenes, each of which is structurally represented by a hierarchy of GameObjects. These GameObjects represent all visible and interactive elements in the 3D virtual environment and can be composed of various components, such as meshes, scripts, and colliders. Unity provides VR application developers with a rich set of tools for implementing immersive interactions. As Unity’s official framework, XRIT [15] can support com- mon VR input modalities such as ray-based selection, di- rect grabbing, teleportation, and gesture recognition. These interactions are typically implemented using component-based 1https://anonymous.4open.science/r/VRExplorer-683A/ scripts attached to objects, while often relying on trigger colliders or physics events. However, the implementation logic varies significantly across projects, especially when third- party packages are used, thereby increasing the complexity of generalizing automated testing across different VR projects. Mono Scripts in Unity-based Application Development. In Unity-based VR application development, Mono scripts constitute the fundamental building blocks for implementing interactive behaviors. These C# classes inherit properties from Unity’s core MonoBehaviour [22] base class, enabling them to leverage Unity’s component-based architecture. Through the Unity Inspector, developers attach these scripts to GameOb- jects to create diverse objects. For example, a gun that shoots bullets can be attached with XRGun, a class inherited from MonoBehaviour, with properties such as a reference to the bullet prefab2, Shoot() function, etc. NavMesh-Based Navigation in Unity. Unity’s NavMesh system [23–25] provides a mechanism to traverse 3D environ- ments using navigation meshes generated from static scene geometry. It supports obstacle avoidance, pathfinding, and dynamic updates, making it suitable for simulating players’ movement. In VR testing, NavMesh can be leveraged to automate scene exploration by guiding a virtual agent through reachable areas. However, it only supports locomotion and requires external control logic to handle task-specific actions, such as interacting with objects or triggering events. III. APPROACH As shown in Fig. 1, the proposed VRExplorer works by first collecting and analyzing open-source VR projects in § III-A. Then, it implements the EAT framework in § III-C after model extraction in § III-B. Next, it conducts testing by VRExplorer in § III-D based on the EAT framework, NavMesh, and PFSM. A. Project Collection and Analysis To comprehensively investigate the interaction behaviors of VR projects, we collect VR open-source project repositories, which represent diversity in terms of application types, interac- tion types, and scene types (more details to be given in § V-A) <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 32 in source list: Liming Nie, Kabir Sulaiman Said, Lingfei Ma, Yaowen Zheng, Yangyang Zhao. "A systematic mapping study for graphical user interface testing on mobile apps", IET Software, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1049/sfw2.12123', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">Based on the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> constructed dataset, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 32 in source list: Liming Nie, Kabir Sulaiman Said, Lingfei Ma, Yaowen Zheng, Yangyang Zhao. "A systematic mapping study for graphical user interface testing on mobile apps", IET Software, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1049/sfw2.12123', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">we conduct a</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> preliminary <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 32 in source list: Liming Nie, Kabir Sulaiman Said, Lingfei Ma, Yaowen Zheng, Yangyang Zhao. "A systematic mapping study for graphical user interface testing on mobile apps", IET Software, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1049/sfw2.12123', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#795AB9" class="#795AB9">analysis</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> and extract the abstraction model consisting of both Object abstraction and Action abstraction, as shown in Fig. 1. After analyzing scenes’ hierarchies and core logic code, we can eliminate non-interactable objects, which have no relations to the core interactable objects (to be tested). These non- interactable objects include static game walls, lighting, the game manager, etc. Further, we deeply dive into the inter- action scripts inherited from the Monobehaviour, via which these interactable objects are attached in the Unity Inspector. Notably, we also consider the interaction scripts developed by the third-party VR interaction libraries that are attached. This in-depth analysis provides insights into VR interactable objects’ key features and facilitates Model Abstraction. 2In Unity, a prefab is a reusable asset template that encapsulates GameOb- jects and components, enabling efficient instantiation and modular design. A Project Collection D VRExplorer Test (§ III-D) 3 & Analysis (§ III-A) Behavior Execution and Scene Exploration Collect By 1 Preliminary Scene Configuration Camera Open Source Repositories Tags & Topics Bake NavMesh Hand Controller Unity-based Collider VR Projects Original Scene Scene with NavMesh (for navigation) VRExplorer Hierarchy Analysis Agent Hierarchy Code Analysis All Scripts 2 Implementation Interface Interaction & Scene Analysis Environment Core Logic Code & Customizing Interactables PFSM Box Third Plugin Analysis Core Scripts Entity Interface Autonomous Configuration Decision Making Event Invocation Task Execution Scripts Implemented extract the Entity Interface Task Execution Autonomous abstraction model Input Next Event Invocation B Model Abstraction (§ III-B) C EAT Framework (§ III-C) Interactable’s Mono Output Task Task Generator Abstraction of Interactable Objects Model Task Generator & Customized Task Model Example of Task Model Triggerable Button, Joystick Parallel and serial combinations Triggerable & Grabbable Base Action Grab-And-Drag-Box Task Gun, Cigarette Lighter Parallel Action Inherit Inherit Move Action Grab Action Grabbable Box, Coin Grab Action Trigger Action Grab Move Action Walk Around Asynchronous Action Abstraction of Actions Grab Base Entity Trigger Pressing Press Pressed Trigger Action Inherit Inherit Pulling Grabbable Entity Triggerable Entity Pull Relaxed + Triggering Time Task Layer Entity Interface C# Scripts File Fire Pulling the trigger + Triggering() Action Layer Triggered + Triggered() Implement Implement Implement Unity Scene GameObjects XR Box Entity Layer XR Joystick XR Button Mono Scripts Third Plugins NavMesh Data Fig. 1: Overview of VRExplorer B. Model Abstraction After project analysis, we conduct model abstraction by generalizing VR interactable objects into abstract objects and classifying common VR interaction behaviors into abstract actions based on OOP, as shown in Part B in Fig. 1. TABLE I: Example of Generalizing VR Interactable Objects VR Interactable Objects Interactable Features Box, Coin, etc. Grabbable Button Triggerable Joystick Transformable Gun, Cigarette Lighter, etc. Grabbable and Triggerable Abstraction of Interactable Objects. We first generalize VR interactable objects into abstract objects containing inter- actable features. Table I gives an example of VR interactable objects with interactable features, which can determine inter- action properties. For instance, objects like boxes and coins can be typically classified as Grabbable, meaning that they can be picked up, while objects like buttons and joysticks are considered Triggerable with two attributes triggering and trig- gered. Notably, some objects may possess multiple interactable features. For instance, a gun or a cigarette lighter possesses both Grabbable and Triggerable attributes, representing com- plex interactions to support not only grabbing but also acting on it functionally, such as shooting or turning on a button. TABLE II: Generalizing Interactions into Abstract Actions VR Interaction Behaviors Move Around, Jump, Fly, etc. Press, Pull, Open/Close, Turn On/Off, Fire(Shoot), etc. Grab, Throw Place, Move Objects Abstract Actions Move Action Trigger Action Grab Action Transform Action Abstraction of Actions. We then classify VR interaction behaviors into abstract actions. Table II shows an example of a VR interactive scene, in which pressing a button, closing or opening a door, turning on or off a lamp, and pulling a joystick, can be conceptualized as compositions of a continuous process with a following event. Specifically, the player’s action – pulling a joystick, can be classified into a continuous pulling process and an event triggered after pulling is over. We classify a behavior with such characteristics into Trigger Action. C. EAT Framework Based on the model abstraction in § III-B, we propose a three-layer framework called EAT, as shown in Part C in Fig. 1. Entity Interface Layer. Based on interactable objects’ abstraction, we define specific interfaces to encapsulate their corresponding attributes. Particularly, we define a base in- terface, called BaseEntity, which includes the Transform property to represent the object’s position, rotation, and scale, as well as the Name property. Thereafter, all other entity interfaces inherit this base interface. For example, an interface Triggerable includes two attributes triggering and triggered with the Enum property. The Entity Interface layer provides customized interfaces tailored to specific features required by the Action layer. Notably, complex interactable features can be efficiently represented through multi-inheritance by multiple interfaces, thereby allowing for higher flexibility and modularity in defining VR interactions. Action Class Layer. We then extract behaviors with iden- tical characteristics into the same action. We first define an abstract base class, namely BaseAction with a virtual asyn- chronous method, Execute(). All other action classes inherit this base class. The Action layer can interact with the Entity Interface Layer and provide functional APIs for VRExplorer to simulate real players’ interactive actions. For example, Trigger Action can be concreted into the TriggerAction class, which Move to 1 Grab-And-Shoot-Gun Task Approach The Gun Move Action Parallel Action Grab Action Move Action Move Action 2 Try to Grab the Gun Trigger Trigger Trigger Action Action Action Move Around 3 Randomly; Try to Fire the Gun Asynchronous Action (a) GASG Process (b) Action Model of GASG task private List<BaseAction> GrabAndShootGunTask( IGrabbableEntity grabbableEntity, ITriggerableEntity triggerableEntity) { List<BaseAction> task = new List<BaseAction>() { new MoveAction(_navMeshAgent, moveSpeed, grabbableEntity.transform.position), new GrabAction(leftHandController, grabbableEntity, new List<BaseAction>() { new ParallelAction(new List<BaseAction>() { new MoveAction(_navMeshAgent, moveSpeed, GetRandomTwitchTarget(transform.position)), new TriggerAction(2.5f, triggerableEntity) }) }) }; return task; } (c) C# Code Snippet of GASG task Fig. 2: Task Instance of Grab-And-Shoot-Gun consists of an asynchronous method Triggering() and a synchronous method Triggered(). Task Model Layer. A composition task consists of multiple parallel and sequential actions. Parallel action executes asyn- chronous methods simultaneously, while sequential actions follow a strict order to complete the previous action before pro- ceeding to the next. The Task Model layer includes a task gen- erator and multiple predefined VR interaction tasks. The task generator creates tasks by accepting either a MonoBehaviour instance or an array of entities (BaseEntity[]). We derive the predefined tasks from commonly used task models. Fig. 2 depicts an example of the Grab-And-Shoot-Gun (GASG) task, which consists of three steps: approaching the table, picking up the gun, and shooting while walking <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 30 in source list: Álvaro Herrero, Cristian I. Pinzón, Emilio Corchado, Javier Bajo. "Chapter 84 Unsupervised Visualization of SQL Attacks by Means of the SCMAS Architecture", Springer Science and Business Media LLC, 2010"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-642-12433-4_84', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">in Fig. 2(a). Fig. 2(b</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">) elaborates on <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 30 in source list: Álvaro Herrero, Cristian I. Pinzón, Emilio Corchado, Javier Bajo. "Chapter 84 Unsupervised Visualization of SQL Attacks by Means of the SCMAS Architecture", Springer Science and Business Media LLC, 2010"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1007/978-3-642-12433-4_84', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> corresponding action model, where the horizontal axis represents the timeline of asynchronous actions starting with Move Action, followed by Parallel Action, within which three actions execute concurrently. The corre- sponding code is also shown in Fig. 2(c). D. VRExplorer Testing As shown in Part D in Figure 1, VRExplorer’s testing process works in three sequential steps: 1 Preliminary Scene Configuration, 2 Implementation Interface, and 3 Scene Exploration with Behavior Execution. 1) Preliminary Scene Configuration: Before scene explo- ration, we facilitate the agent’s navigation capability based on Unity’s NavMeshAgent [24] via NavMesh baking. We first configure terrain objects, such as floors and walls, to be static. Regarding those objects that may dynamically move, such as doors, we attach the NavMesh Obstacle [25] component with the enabled Carve option, thereby allowing them to dynamically modify NavMesh. Fig. 3 depicts an example of opening and closing dynamic doors. (a) Doors closed, separating two (b) Doors opened, enabling rooms’ NavMesh NavMesh connection Fig. 3: Dynamic doors with a NavMesh Obstacle component and the Carve option enabled 2) The Entity Interface Layer: On top of the EAT frame- work, implementing interactable interfaces is the core to tackle two challenges: (1) lack of generalizability caused by diverse Unity versions and the fragmentation of the VR development ecosystem; and (2) intricate scene exploration and VR interac- tions. Using interfaces to encapsulate implementation details can enable testing for a diversity of VR applications. Procedure 1 Entity Interface Layer Implementation Example. 1: class XRGun implements ITriggerable, IGrabbable: 2: property TriggeringTime ← 1.5 3: property Name ← “Gun” 4: property Grabbable: 5: if component exists then return it 6: else add new Grabbable component 7: method Triggerring() do nothing 8: method Triggerred() calls Fire() 9: method OnGrabbed() do nothing 10: field projectilePrefab: GameObject 11: field startPoint: Transform 12: field launchSpeed: float 13: method Fire(): 14: Instantiate projectile and apply forces 15: method ApplyForce(rigidbody): 16: Calculate and apply physics forces To ensure coverage and verify the reliability of interaction methods, test engineers need to select and customize the appropriate Entity layer interfaces for the Mono scripts related to the core VR interaction logic in the target project with the corresponding interface functions implemented. Take Proce- dure 1 as an example3, in which a gun class XRGun possessing both Grabbable and Triggerable features, can be implemented by inheriting MonoBehaviour while simultaneously imple- menting the IGrabbable and ITriggerable interfaces. This design allows the gun object to be both grabbed and triggered during VR interactions without additional script components. This approach allows XRGun to seamlessly integrate multi- ple interaction capabilities (i.e., grabbable and triggerable). Leveraging interface composition, we can flexibly define and extend object behaviors without modifying the base class hierarchy, thereby promoting code reusability and achieving modular design in the VR interaction system. 3) Scene Exploration with Behavior Execution: After com- pleting the scene configuration and implementing interactable interfaces, VRExplorer automatically performs scene explo- ration and VR interactions by executing the correspond- 3The implementation of the entity interface’s code lines is marked blue. …… Actual Transition Probable Transition 1 𝑇𝑖: Grab-and-Shoot-Gun 𝑝 𝐸𝑖: Release Speed-Up Skills 2𝑝 𝑇𝑖+1: Press-Button 𝐸𝑖+1: Release 3 1 − 𝑝 Slow-Time Skills …… …… Fig. 4: Example of PFSM State Transition in a VR Scene. ing behaviors. We design two types of behaviors for VR- Explorer: task execution and autonomous event invocation. When executing tasks, VRExplorer perceives the scene and inputs MonoBehaviour instances or an array of entities (BaseEntity[]) into the task generator to obtain and execute the corresponding tasks. Notably, autonomous events reveal that players in VR environments not only interact with objects but also invoke autonomous events, such as casting skills to gain acceleration buffs. PFSM. To comprehensively cover such testing cases, VREx- plorer maintains a configurable list of UnityEvents, allowing test engineers to easily customize the functions to be covered by configuring them in the Inspector before testing. These two types of behaviors constitute the behavior space, where each <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 26 in source list: Karasek, Daniel. "SuperB: Superior Behavior-based Anomaly Detection Defining Authorized Users\u27 Traffic Patterns", DigitalCommons@Kennesaw State University, 2020"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=1475451734&n=3793&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">behavior can be regarded as a</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> node. The <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 26 in source list: Karasek, Daniel. "SuperB: Superior Behavior-based Anomaly Detection Defining Authorized Users\u27 Traffic Patterns", DigitalCommons@Kennesaw State University, 2020"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=1475451734&n=3793&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">behavior</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> space itself forms <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 26 in source list: Karasek, Daniel. "SuperB: Superior Behavior-based Anomaly Detection Defining Authorized Users\u27 Traffic Patterns", DigitalCommons@Kennesaw State University, 2020"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=1475451734&n=3793&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">a</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> directed graph composed of these nodes. For behavior decision-making, VRExplorer employs PFSM to determine transitions between nodes, thereby defining the topology of the directed graph. We use T = {T1, T2, . . . , TN } to denote the set of all task states, where Ti represents the i-th task state in the execution sequence, with i ∈ {1, 2, . . . , N }. Similarly, we use E = {E1, E2, . . . , EM } to denote the set of exploration states, where Ej represents the j-th exploration state in the execution sequence, with j ∈ {1, 2, . . . , M }. At each decision point in PFSM, a variable determines the transition direction. The probability of transitioning to an exploration state (Ej) is denoted by p while the probability of transitioning to a task state (Ti) is (1 − p). Fig. 4 depicts an example of state transition in a VR scene, where Speed-Up Skills node and Slow-Time Skills node are two example nodes of an autonomous event adding buffs to the player, while GASG node and Press-Button (PB) node are two example nodes of the task. Those four nodes are independent of each other, while the PFSM is responsible for decision-making and state transitions among them. Table III lists all the state transitions. TABLE III: State Transition Table of PFSM Current State Next State Condition Probability Ti <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 4 in source list: Bardakov, Valeriy, Nasybullov, Timur. "Multi-switches and representations of braid groups", 2019"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=286804274&n=3793&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">Ti+1 Ti Ej</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Ej Ej+<img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 4 in source list: Bardakov, Valeriy, Nasybullov, Timur. "Multi-switches and representations of braid groups", 2019"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=286804274&n=3793&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">1</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Ej <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 4 in source list: Bardakov, Valeriy, Nasybullov, Timur. "Multi-switches and representations of braid groups", 2019"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=86.5214306965019&svr=6&lang=zh_hans&sid=286804274&n=3793&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">Ti+1 Ti Ej</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Ti+1 Ej+1 Default Default Default Default E exhausted T exhausted 1 − p p p 1 − p 1.0 1.0 Path-finding for Navigation. In the path-finding process for navigation, we implement two algorithms: (i) a Greedy al- gorithm, which follows a local optimization strategy based on the shortest path principle, and (ii) a Backtracking algorithm with Pruning, which searches for globally optimal solutions. Since the Greedy algorithm significantly reduces the time complexity and aligns well with the real-time requirements of VR application testing, we mainly adopt it in our experiments. In summary, VRExplorer continuously obtains scene infor- mation, decides the current behavior to execute, and receives feedback after performing the behavior. This process is re- peated until all interactable objects and events are covered. IV. EVALUATION OF VRExplorer A. Implementation Attach Predefined Mono Script XRTriggerable to a Button GameObject Fig. 5: Predefined Mono Script Component XRTriggerable Implementation of the EAT Framework. We de- rive Grabbable, Transformable, and Triggerable in- terfaces from the base interface BaseEntity in the En- tity layer. In the Action layer, we define GrabAction, MoveAction, ParallelAction, TransformAction, and TriggerAction as the subclasses of the abstract base class BaseAction. Each subclass overrides the Execute() method to implement its corresponding behavior. Thereafter, we pro- vide the implementations of common task models as the code- level compositions of actions, e.g., the PB and GASG tasks, as shown previously in Fig. 2(c). Implementation of the Entity Interface layer. For each project, we analyze the interactable objects in scenes and all the C# scripts referenced by the objects. We select codes that are relevant to the VR interaction logic, they are also called Core Code. We then modify scripts (classes) in Core Code to implement the interfaces of the Entity Interface layer (measures to diminish this threat to internal validity to be given in § VI). To mitigate the additional workloads caused by implementing such interfaces on developers, we provide a set of predefined scripts inherited from MonoBehaviour for the projects developed by the XRIT framework. These commonly used Mono scripts have already been implemented in the corresponding interfaces. Therefore, we can also attach the predefined Mono C# script (as given in § III-D2). Take Fig. 5 as an example, in which test engineers can simply add the predefined Mono scripts that we provide onto the target objects and flexibly configure the functions (methods) under test by adding events to UnityEvent [26] in the Unity Inspector. This design can be easily extended to diverse interactions without introducing too many additional workloads to developers. Implementation of VRExplorer. We equip the VREx- plorer’s GameObject with a NavMeshAgent component for navigation on NavMesh with the attached controller com- ponent (like a player’s hands) to enable interactions with objects. To support scene perception, we implement the EntityManager class, which is responsible for registering scene information and providing APIs for scene analysis within VRExplorer. Then, we implement PFSM to support conditional branching for decision-making, where PFSM’s inputs are derived from scene analysis. Next, we implement a TaskGenerator component to support task execution by translating relevant inputs into the corresponding task model. Environment. We implement and evaluate VRExplorer with the comparison of other baselines (in § IV-C) on a computer with <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 14 in source list: Submitted to University of Bristol  on 2022-10-11"><a href="javascript:void(0);" onClick="window.open('/paperInfo.asp?r=73.6689509042577&svr=6&lang=zh_hans&oid=oid:2:707547647&n=2&perc=0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">AMD Ryzen 7 5800H with Radeon Graphics CPU</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, 32GB RAM, <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 14 in source list: Submitted to University of Bristol  on 2022-10-11"><a href="javascript:void(0);" onClick="window.open('/paperInfo.asp?r=73.6689509042577&svr=6&lang=zh_hans&oid=oid:2:707547647&n=2&perc=0', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">and NVIDIA GeForce RTX 3060 GPU</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> (6GB) Graphics card. This computer is installed with MS Windows 11 and the same Unity version as that used in all evaluated projects. System Configuration Parameters. Since the proposed VRExplorer simulates a player to explore the VR scene, the initial position, the move speed (MS), and the turn speed (TS) (also called rotation speed in [11]) greatly affect the testing performance. In particular, we let the center of the scene be the initial position. Regarding the move speed, excessive speed can introduce physical and interactive issues (e.g., test tools may unintentionally move out of a platform due to inertia), although a faster speed may generally improve performance. Therefore, to balance efficiency with practicality, we choose MS = 6 m/s and TS = 60 deg/s as the standard parameters for VRGuide and VRExplorer in all subsequent experiments. § VI discusses the rationale for these settings. Since these parameters are chosen to be the same for all test tools, they are not repeated in detail in the subsequent experimental results. The experiment is terminated when the test tool reaches convergence. In the PFSM model, the probability of transitioning to the next state is decided by the parameter p, which is set to 0.5, indicating the same propensity for both state transitions. B. Metrics With reference to [11, 27–29], we consider executable lines of code (ELOC) coverage, method coverage, interactable objects coverage, and convergence time as evaluation metrics: • ELOC Coverage. Since ELOC mainly focuses on code lines containing executable programs, ELOC coverage mea- sures the percentage of these executable lines with the ex- clusion of blank lines, comments, and declaration statements during testing. We adopt Code Coverage [30], an official tool provided by Unity, to automatically record the ELOC coverage of C# scripts. Running in Unity Editor Mode, the testing tool exports the coverage data as both a historical report in .html format and summary data in .xml format. • Method Coverage. Besides ELOC coverage, we also con- sider method coverage to evaluate the performance. The method coverage quantifies the percentage of testing methods (functions) that have been invoked during testing. • Interactable Object Coverage (IO Coverage). To fairly compare the proposed VRExplorer with SOTA baseline VR- Guide [11], we also consider IO coverage. However, slightly different from [11], in which interactable objects only in- clude objects that can receive mouse “click” events, we further extend interactable objects to all objects that can be triggered, grabbed, moved, and transformed. These inter- actable objects are identified by analyzing and confirming the MonoBehaviour scripts associated with the scene objects. • Convergence Time. Rather than considering the coverage performance, we also evaluate the efficiency of the proposed approach. Particularly, we consider convergence time to mea- sure how fast a tool can reach the converged state, which is defined as the status at which the testing tool no longer seeks additional scene exploration. The convergence time refers to the amount of time taken for the testing tool to reach the converged state from the initial state. C. Baselines Given <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 13 in source list: http://export.arxiv.org/pdf/2308.06783"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=2784694155&n=3805&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">the unique</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> characteristics <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 13 in source list: http://export.arxiv.org/pdf/2308.06783"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=2784694155&n=3805&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">of VR applications, such as</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> complex VR interactions <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 13 in source list: http://export.arxiv.org/pdf/2308.06783"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=2784694155&n=3805&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#935F32" class="#935F32">and</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the fragmented nature of the VR development ecosystem, current automated game testing tools and Android application testing frameworks are not suitable for VR application testing. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 16 in source list: Xiongfei Wu, Jiaming Ye, Ke Chen, Xiaofei Xie, Yujing Hu, Ruochen Huang, Lei Ma, Jianjun Zhao. "Widget Detection-based Testing for Industrial Mobile Games", 2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE-SEIP58684.2023.00021', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">To the best of our knowledge</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, VRGuide [11] <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 16 in source list: Xiongfei Wu, Jiaming Ye, Ke Chen, Xiaofei Xie, Yujing Hu, Ruochen Huang, Lei Ma, Jianjun Zhao. "Widget Detection-based Testing for Industrial Mobile Games", 2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE-SEIP58684.2023.00021', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">is the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> SOTA <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 16 in source list: Xiongfei Wu, Jiaming Ye, Ke Chen, Xiaofei Xie, Yujing Hu, Ruochen Huang, Lei Ma, Jianjun Zhao. "Widget Detection-based Testing for Industrial Mobile Games", 2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE-SEIP58684.2023.00021', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">automated testing</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> approach <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 16 in source list: Xiongfei Wu, Jiaming Ye, Ke Chen, Xiaofei Xie, Yujing Hu, Ruochen Huang, Lei Ma, Jianjun Zhao. "Widget Detection-based Testing for Industrial Mobile Games", 2023 IEEE/ACM 45th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE-SEIP58684.2023.00021', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#63009c" class="#63009c">for</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> VR applications. Besides VRGuide, VRTest [10] is a previ- ous version, which nevertheless has inferior performance to VRGuide in terms of both method coverage and IO coverage (only pointer click events received). V. EXPERIMENTAL RESULTS We conduct extensive experiments to evaluate VRExplorer and aim to investigate three <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 20 in source list: https://ar5iv.org/html/2301.12453"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=116281440&n=3811&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">research questions</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> (RQs): • <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 20 in source list: https://ar5iv.org/html/2301.12453"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=116281440&n=3811&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">RQ1: How does</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> VRExplorer <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 20 in source list: https://ar5iv.org/html/2301.12453"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=116281440&n=3811&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">perform with</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> comparison <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 20 in source list: https://ar5iv.org/html/2301.12453"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=116281440&n=3811&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#336699" class="#336699">of the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> existing SOTA approach in terms of various performance metrics in diverse VR projects? • RQ2: How do different modules contribute to the perfor- mance of VRExplorer? • RQ3: Can VRExplorer detect real-world VR bugs? A. VRExplorer Performance Constructing VR Projects for Evaluation. To evaluate the proposed VRExplorer, we construct a project dataset4, consisting of nine representative VR projects, as summarized in Table IV. This project dataset can be divided into two groups: (1) Group 1, in which we select the most complex three projects also being included in VRGuide [11], and (2) Group 2, into which we introduce the other six Unity-based VR projects developed by more recent versions (e.g., after 2020.x). It is worth noting that we construct Group 1 (with relatively older Unity versions) primarily for a fair comparison with VRGuide, as it only supports the “click” interaction. Compared with Group 1, Group 2 contains VR projects developed by recent Unity versions, which can support more diverse interactions (while not being supported in VRGuide). The nine projects selected for experiments represent a diverse and comprehensive subset of VR applications. Firstly, 4List of constructed project dataset can be found at https://anonymous. 4open.science/r/VRExplorer-683A/Artifacts/Evaluated Repo Url.md TABLE IV: Quantitative Metrics of Selected VR Projects Projects # of Scripts LOC # of Files Scenes # of GOs Version Group 2 Group 1 UnityVR unity-vr-maze UnityCityView EscapeGameVR VR-Basics VGuns EE-Room* VR-Room VR-Adventure 150 24,858 158 25,261 182 28,335 91 6,659 62 2,677 81 10,900 88 4,450 65 3,660 330 3 212 1 446 34 1,377 44 724 5 848 36 1,063 8 679 2 124 2019.x 278 5.x 1,194 2019.x 8,256 2021.x 2,143 2021.x 1,653 2020.x 1,517 2020.x 414 2022.x 11 260 91 2 288 2022.x * EE-Room stands for Edutainment-Escape-Room. the chosen projects cover all of the most commonly featured VR application types as defined in [31]: (i) Action and Shooter (VGuns), (ii) Simulation (VR-Basisc, VR-Room, UnityVR), (iii) Adventure (unity-vr-maze, VR-Adventure), and (iv) Puzzle (Edutainment-Escape-Room, EscapeGameVR). This diversity ensures that the experimental results are generalizable across different VR genres and interaction manners. Second, these projects cover a wide range of Unity versions, from older releases like 2019.4.2f1 (UnityCityView) to more recent versions such as 2022.3.7f1 (VR-Adventure), as well as the legacy version 5.4.1f1 (developed for unity-vr-maze). This version diversity ensures that our testing approach is evaluated across different Unity engine environments, demonstrating its robustness and compatibility. Moreover, the selected projects exhibit a variety of scales and complexities. For instance, the number of C# scripts ranges from as few as 11 in VR-Adventure to over 90 in EscapeGameVR, with LOC spanning from around 260 to over 6,600. This selection allows us to assess the testing tools’ versatility from small and medium-sized VR applications to large ones. These projects also differ in the number of scenes and the number of GameObjects (GOs), with some projects like EscapeGameVR having 44 scenes, reflecting complex and rich environments, while others like VR-Room have fewer scenes but have poten- tially dense and complicated interactions. Results on Projects of Group 1. Fig. 6 shows the trend of ELOC coverage versus time when testing all the projects in Group 1. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 5 in source list: http://arxiv.org/pdf/2408.08515"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=2770836417&n=3809&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">Table V shows the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> experimental <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 5 in source list: http://arxiv.org/pdf/2408.08515"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=2770836417&n=3809&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">results of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> VREx- plorer <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 5 in source list: http://arxiv.org/pdf/2408.08515"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=2770836417&n=3809&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">with the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> comparison of the baseline approach VRGuide on the three projects in Group 1. To quantitatively evaluate the performance improvement of VRExplorer over VRGuide (or other compared methods in § V-B), we define the performance gain of method A over method B as follows, GAB = PA − PB PB × 100%, (1) where PA and PB denote the performance metrics achieved by methods A and B, respectively. For example, we evaluate the ELOC performance gain of VRExplorer over VRGuide in project unity-vr-maze by GEG = PEP−GPG × 100% = 81.67−66.53 66.53 × 100% = 22.8%. Similarly, we have the perfor- mance gain of VRExplorer (E) over VRGuide (G) in terms of method coverage and IO coverage by 16.7% and 6.1%, respectively. Meanwhile, our VRExplorer reduces the conver- gence time cost by 43.9% compared with VRGuide. For project UnityCityView, VRExplorer achieves performance gains <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, Zhendong Su. "Practical GUI Testing of Android Applications Via Model Abstraction and Refinement", 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE.2019.00042', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">in terms of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> ELOC <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, Zhendong Su. "Practical GUI Testing of Android Applications Via Model Abstraction and Refinement", 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE.2019.00042', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">coverage, method coverage, and</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> IO <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, Zhendong Su. "Practical GUI Testing of Android Applications Via Model Abstraction and Refinement", 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE.2019.00042', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">coverage</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> over VRGuide by 36.3%, 27.6%, and 66.7%, respectively. Our 100 100.00% 94.29% Coverage (%) 80 60 40 20 0 81.67% 43.82% 66.53% VRExplorer ELOC Coverage VRExplorer Interactable Coverage VRGuide ELOC Coverage VRGuide Interactable Coverage 0 25 50 75 Time (s) 100 125 150 (a) Coverage versus Time in Project unity-vr-maze 100 VRExplorer ELOC Coverage 100.00% 92.22% Coverage (%) 80 VRExplorer Interactable Coverage 60 VVRRGGuuiiddee EInLtOerCacCtoavbelreaCgoeverage 67.66% 60.00% 40 20 20.36% 0 0 20 40Time (s) 60 80 (b) Coverage versus Time in Project UnityCityView 100 VRExplorer ELOC Coverage 100.00% 100.00% Coverage (%) 80 VRExplorer Interactable Coverage VRGuide ELOC Coverage 75.93% 60 VRGuide Interactable Coverage 64.81% 40 20 18.52% 0 0 2 4 Time (s) 6 8 (c) Coverage versus Time in Project UnityVR Fig. 6: ELOC Coverage∗ versus Time during the Testing Process of projects in Group 1. *Initial ELOC Coverage is identical across projects as initialization code (e.g., Awake()) runs immediately, regardless of interactions. VRExplorer achieves consistent performance gains in project UnityVR, e.g., ELOC coverage gains by 17.1% and method coverage gain by 9.1%, with the reduced convergence time by 12.5%. Notably, our VRExplorer achieves 100% IO coverage in all three projects while VRGuide can only achieve 100% IO coverage in project UnityVR. In summary, experimental results demonstrate that the proposed VRExplorer consistently outperforms VRGuide in coverage performance and efficiency. Results on Projects of Group 2. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 24 in source list: Jiayuan Liang, Sinan Wang, Xiangbo Deng, Yepang Liu. "RIDA: Cross-App Record and Replay for Android", 2023 IEEE Conference on Software Testing, Verification and Validation (ICST), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICST57152.2023.00031', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">Table VI shows the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> exper- imental <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 24 in source list: Jiayuan Liang, Sinan Wang, Xiangbo Deng, Yepang Liu. "RIDA: Cross-App Record and Replay for Android", 2023 IEEE Conference on Software Testing, Verification and Validation (ICST), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICST57152.2023.00031', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">results of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> VRExplorer with <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 24 in source list: Jiayuan Liang, Sinan Wang, Xiangbo Deng, Yepang Liu. "RIDA: Cross-App Record and Replay for Android", 2023 IEEE Conference on Software Testing, Verification and Validation (ICST), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICST57152.2023.00031', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> comparison with <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 24 in source list: Jiayuan Liang, Sinan Wang, Xiangbo Deng, Yepang Liu. "RIDA: Cross-App Record and Replay for Android", 2023 IEEE Conference on Software Testing, Verification and Validation (ICST), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICST57152.2023.00031', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:brown" class="brown">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> baseline approach VRGuide on the six representative projects in Group 2 in terms of ELOC coverage and method cover- age. Notably, we omit the IO coverage here mainly because VRGuide exhibited lower IO coverage than our VRExplorer due to its sole “clicking” interaction. Therefore, we primarily focus on the code coverage (ELOC and method coverage) of the core development logic of VR projects. We observe that our VRExplorer consistently outper- forms VRGuide in all six projects. In particular, for project VR-Basics, VRExplorer achieves ELOC coverage and method coverage gains over VRGuide by 93.8% and 72.8%, re- spectively. For project VR-Room, VRExplorer achieves ELOC coverage and method coverage gains by 89.4% and 65.0%, re- spectively. It is worth noting that VRExplorer achieves ELOC coverage and method coverage gains by 170.7% and 100.0%, respectively, in project VGuns, showcasing its strength in handling more complex and diverse VR testing scenarios. Comprehensively considering experimental results in all the nine projects in both Group 1 and Group 2, VRExplorer has achieved average ELOC coverage gain by 72.8% and average TABLE V: Results on Projects in Group 1 Projects Approaches ELOC Coverage (%) Method Coverage (%) IO Coverage(%) Convergence Time Cost (s) # of Interactable Objects unity-vr-maze UnityCityView UnityVR VRGuide VRExplorer VRGuide VRExplorer VRGuide VRExplorer 66.53 81.67 (+22.8%) 67.66 70.59 82.35 (+16.7%) 78.38 94.29 100.00 (+6.1%) 60.00 145.0 81.4 (-43.9%) 45.0 35 92.22 (+36.3%) 64.81 84.62 100.00 8.8 100.00 (+27.6%) 100.00 (+66.7%) 89.3 (+98.4%) 15 75.93 (+17.1%) 92.31 (+9.1%) 100.00 7.7 (-12.5%) 3 TABLE VI: Results on Projects of Group 2 Projects Approaches ELOC Coverage (%) Method Coverage (%) VR-Basics VR-Room VGuns VR-Adventure EE-Room EscapeGameVR VRGuide VRExplorer VRGuide VRExplorer VRGuide VRExplorer VRGuide VRExplorer VRGuide VRExplorer VRGuide VRExplorer 41.38 80.17 (+93.8%) 40.97 53.22 91.93 (+72.8%) 50.63 77.61 (+89.4%) 28.68 83.54 (+65.0%) 38.89 77.57 (+170.7%) 54.12 77.78 (+100.0%) 65.00 91.76 (+69.6%) 38.08 95.00 (+46.2%) 58.06 70.61 (+85.5%) 41.77 88.17 (+51.8%) 55.26 71.08 (+70.2%) 73.68 (+33.3%) method coverage gain by 46.9% compared to VRGuide. Ex- perimental results demonstrate that VRExplorer outperforms VRGuide (in diverse performance metrics) consistently across various VR projects, which were developed with different versions of the Unity engine. Answer to RQ1: Experimental results on all nine rep- resentative VR projects demonstrate that VRExplorer out- performs the SOTA approach VRGuide <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, Zhendong Su. "Practical GUI Testing of Android Applications Via Model Abstraction and Refinement", 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE.2019.00042', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">in terms of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> ELOC <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, Zhendong Su. "Practical GUI Testing of Android Applications Via Model Abstraction and Refinement", 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE.2019.00042', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">coverage, method coverage, and</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> IO <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, Zhendong Su. "Practical GUI Testing of Android Applications Via Model Abstraction and Refinement", 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE.2019.00042', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">coverage</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. In particular, VRExplorer achieves the average performance gains over VRGuide <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, Zhendong Su. "Practical GUI Testing of Android Applications Via Model Abstraction and Refinement", 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE.2019.00042', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">in terms of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> ELOC <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, Zhendong Su. "Practical GUI Testing of Android Applications Via Model Abstraction and Refinement", 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE.2019.00042', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">coverage</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> and <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 8 in source list: Tianxiao Gu, Chengnian Sun, Xiaoxing Ma, Chun Cao, Chang Xu, Yuan Yao, Qirun Zhang, Jian Lu, Zhendong Su. "Practical GUI Testing of Android Applications Via Model Abstraction and Refinement", 2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE), 2019"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE.2019.00042', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">method coverage</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> by 72.8% and 46.9%, respectively, across all the projects. Moreover, VRExplorer converges faster or comparably faster than VRGuide while maintaining substantially higher cover- age. These performance improvements are observed across diverse real-world VR scenarios, demonstrating the proposed VRExplorer’s strong generalizability in covering code and interactable VR objects during automated testing. B. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 25 in source list: Xinglong Yin, Mengxi Zhang, Tengmei Wang, Huaxiao Liu. "Efficient exploration of hard-to-find function in GUIs: A method and best practice", Displays, 2025"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.displa.2025.103037', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">Ablation Study To investigate the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> contribution <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 25 in source list: Xinglong Yin, Mengxi Zhang, Tengmei Wang, Huaxiao Liu. "Efficient exploration of hard-to-find function in GUIs: A method and best practice", Displays, 2025"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.displa.2025.103037', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">of each</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> module in <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 25 in source list: Xinglong Yin, Mengxi Zhang, Tengmei Wang, Huaxiao Liu. "Efficient exploration of hard-to-find function in GUIs: A method and best practice", Displays, 2025"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.displa.2025.103037', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#B64B01" class="#B64B01">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> EAT framework of the proposed VRExplorer, we conduct a comprehensive ablation study by selectively removing one interaction module from VRExplorer. We then evaluate these methods by comparing them with the full-fledged VRExplorer framework in terms of ELOC coverage and method coverage. Notably, we also compare them with VRGuide to investigate the performance improvement contributed by each module. Ablation Experiment Configuration. As described in § III-C, interaction behaviors play a crucial role in the EAT framework. To investigate these interaction behaviors, we remove a specific interaction from a tested project’s modules (provided that this module is used in this project). For example, we obtain VRExplorer w/o T by removing the Triggerable module from the Entity layer’s interface Triggerable, the Action layer’s class TriggerAction, all tasks involve trigger- TABLE VII: Results of Ablation Study Projects Approaches ELOC Coverage (%) Method Coverage (%) VRGuide VR-Basics VRExplorer VRExplorer w/o T VRExplorer w/o Tf 41.38 80.17 68.10 (-15.0%) 59.24 (-26.1%) 53.22 91.93 77.42 (-15.9%) 70.00 (-16.2%) VR-Room VRGuide VRExplorer 40.97 77.61 50.63 83.54 VRExplorer w/o G 58.52 (-24.6%) 69.62 (-16.4%) VRExplorer w/o T 64.12 (-17.3%) 67.00 (-19.7%) VGuns VRGuide VRExplorer VRExplorer w/o TG VRExplorer w/o AE 28.68 77.57 50.37 (-35.3%) 65.07 (-16.1%) 38.89 77.78 61.11 (-16.7%) 63.89 (-17.9%) ing in the Task layer, and the corresponding Mono C# scripts XRTriggerable.cs. Similarly, we obtain VRExplorer’s other ablated variants: VRExplorer w/o Tf (without Transformable), VRExplorer w/o G (without Grabbable), VRExplorer w/o TG (without both Triggerable and Grabbable)), and VRExplorer w/o AE (without autonomous event). Results of <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Oluwadamilare Harazeem Abdulganiyu, Taha Ait Tchakoucht, Ahmed El Hilali Alaoui, Yakub Kayode Saheed. "Attention-Driven Multi-Model Architecture for Unbalanced Network Traffic Intrusion Detection via Extreme Gradient Boosting", Intelligent Systems with Applications, 2025"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.iswa.2025.200519', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">Ablation Study. Table</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> VII <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Oluwadamilare Harazeem Abdulganiyu, Taha Ait Tchakoucht, Ahmed El Hilali Alaoui, Yakub Kayode Saheed. "Attention-Driven Multi-Model Architecture for Unbalanced Network Traffic Intrusion Detection via Extreme Gradient Boosting", Intelligent Systems with Applications, 2025"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.iswa.2025.200519', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">presents the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> exper- imental <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Oluwadamilare Harazeem Abdulganiyu, Taha Ait Tchakoucht, Ahmed El Hilali Alaoui, Yakub Kayode Saheed. "Attention-Driven Multi-Model Architecture for Unbalanced Network Traffic Intrusion Detection via Extreme Gradient Boosting", Intelligent Systems with Applications, 2025"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.iswa.2025.200519', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">results of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> the <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Oluwadamilare Harazeem Abdulganiyu, Taha Ait Tchakoucht, Ahmed El Hilali Alaoui, Yakub Kayode Saheed. "Attention-Driven Multi-Model Architecture for Unbalanced Network Traffic Intrusion Detection via Extreme Gradient Boosting", Intelligent Systems with Applications, 2025"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.iswa.2025.200519', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">ablation study</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> in terms of <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 17 in source list: Oluwadamilare Harazeem Abdulganiyu, Taha Ait Tchakoucht, Ahmed El Hilali Alaoui, Yakub Kayode Saheed. "Attention-Driven Multi-Model Architecture for Unbalanced Network Traffic Intrusion Detection via Extreme Gradient Boosting", Intelligent Systems with Applications, 2025"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.iswa.2025.200519', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#A85503" class="#A85503">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> ELOC coverage and method coverage in three projects: VR-Basics, VR-Room, and VGuns, where the best result is highlighted with an underline. These ablation experiments cover all the VRExplorer’s variants and span all the Unity versions of the projects in Group 2 (from 2020 to 2022). Specifically, we compare VRGuide and VRExplorer with its five ablated variants: VRExplorer w/o G, VRExplorer w/o T, VRExplorer w/o Tf, VRExplorer w/o TG, and VRExplorer w/o AE. To quantitatively evaluate the effect of removing each module, we also calculate the performance gain of a method over another compared method according to Eq. (1). In project VR-Basics, the full-fledged VRExplorer <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 29 in source list: https://zhendong2050.github.io/res/time-travel-testing-21-01-2020.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=3084761505&n=3811&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">achieves the best performance in terms of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> 80.17% ELOC <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 29 in source list: https://zhendong2050.github.io/res/time-travel-testing-21-01-2020.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=3084761505&n=3811&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">coverage</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> and 91.93% method coverage. We observe that removing the Triggerable module causes a 15.0% ( 68.1800−.1870.17 × 100%) decrease in ELOC coverage and a 15.9% decrease in method coverage and the removal of the Transformable module leads to a 26.1% decrease in ELOC coverage and 16.2% in method coverage. In project VR-Room, we also find that the removal of the Grabbable module has the most pronounced effect, i.e., reducing ELOC coverage by 24.6% and method coverage by 16.4%. Disabling the Triggerable module also leads to an ELOC decrease of 17.3% and a method coverage decrease of 19.7%. In project VGuns, we observe that the removal of both the Triggerable and Grabbable modules causes a 35.3% decrease in ELOC coverage and a 16.7% decrease in method coverage. Moreover, we find that the removal of the autonomous event module causes a 16.1% decrease in Fig. 7: A Detected Bug in EscapeGameVR ELOC coverage and a 17.9% decrease in method coverage, indicating that the autonomous event module we described in § III-D3 also contributes significantly. These results confirm the significant contribution of each interaction module to VRExplorer despite various portions across different projects. Overall, removing any individual module leads to a sub- stantial drop in code coverage (in terms of ELOC coverage and method coverage), further confirming the importance of integrating all three interaction-aware modules in the EAT framework. The full-fledged VRExplorer consistently yields the highest code coverage. Answer to RQ2: Removing any module from the EAT framework leads to a substantial decrease in ELOC coverage and method coverage. The degree of impact caused by the ablated module varies by diverse projects and interaction types. These results validate <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: https://arxiv.org/html/2407.03037v1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=3933262650&n=3808&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">the necessity and effectiveness of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> integrating all <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: https://arxiv.org/html/2407.03037v1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=3933262650&n=3808&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">the modules</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> in <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 7 in source list: https://arxiv.org/html/2407.03037v1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=3933262650&n=3808&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> VRExplorer for com- prehensive interaction-aware VR testing. C. Bug Detection in VR Projects Since <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: https://arxiv.org/html/2307.07221v2"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=1589945964&n=3807&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">the primary goal of software testing is to</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> detect or <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 3 in source list: https://arxiv.org/html/2307.07221v2"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=73.6689509042577&svr=6&lang=zh_hans&sid=1589945964&n=3807&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">identify</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> potential bugs, we also investigate whether the pro- posed VRExplorer can detect real-world bugs in VR projects. We have scanned all nine projects through comprehensive testing conducted by VRExplorer. Thereafter, we have suc- cessfully detected two functional bugs and one non-functional bug in total, from projects EscapeGameVR, UnityCityView, and EscapeGameVR. Herein, a functional bug refers to a bug that causes the VR application not to work as expected, while a non-functional bug denotes a bug not directly related to the functionality (may be relevant to performance and usability issues). Notably, two functional bugs have been found in projects EscapeGameVR and UnityCityView while we have detected one non-functional bug in project EscapeGameVR. Moreover, the bug from UnityCityView has been fixed by developers (after checking the commits), while two bugs from EscapeGameVR reported by us have not been confirmed by developers so far. Our VRExplorer can detect all these VR bugs, which are either functional or non-functional. Although VRGuide <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: Xiaoyin Wang, Tahmid Rafi, Na Meng. "VRGuide: Efficient Testing of Virtual Reality Scenes via Dynamic Cut Coverage", 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ASE56229.2023.00197', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">is able to detect the bug in</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> project <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: Xiaoyin Wang, Tahmid Rafi, Na Meng. "VRGuide: Efficient Testing of Virtual Reality Scenes via Dynamic Cut Coverage", 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ASE56229.2023.00197', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">UnityCityView</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, it misses the other two bugs in project EscapeGameVR. Fig. 7 shows a screenshot of one functional bug detected by our VRExplorer in project EscapeGameVR. This bug oc- curs due to assignment exception, which can be explained as follows. When a bow releases an arrow, the method ReleaseArrow() is invoked with the attempt to access the arrowSpawnPoint field of the ArrowController class. However, the developers have mistakenly failed to assign a value to this field (or the assignment was unintention- ally lost), thereby resulting in an Unassigned Reference Excep- tion [32, 33] bug, which is a Unity-specific runtime exception occurring when a serialized reference field (typically declared as public or marked with [SerializeField] [34]) in a MonoBehaviour class is accessed without being assigned a value in the Unity Inspector. Another non-functional bug in EscapeGameVR is caused by the loss of the ArrowPrefab resource. This bug can be identified by our VRExplorer through a simple manual inspection. The reason why these two bugs are not detected by VRGuide is that triggering these two bugs requires complicated actions and multiple types of interactions, while VRGuide cannot handle them. Differently, our VRExplorer can complete this difficult testing task through our model-based framework. In particular, VRExplorer requires the correct placement of three cubes onto platforms of the corresponding color, thereby ensuring that none of them fall outside the designated plat- form areas. Once this condition is satisfied, a new bow is instantiated. Consequently, triggering the bug needs the bow to be drawn and released by VRExplorer. This type of complex interactive task contains two or even more interactive patterns (e.g., Transformable, Grabbable, and Triggerable), which can not be completed by VRGuide (only supporting “clicking” interaction), while our approach handles this task properly. Answer to RQ3: Through testing VR projects, the proposed VRExplorer can successfully detect three real-world bugs in VR projects. More importantly, our VRExplorer is capable of detecting complex VR bugs, which can only be triggered after complex interactions. VI. DISCUSSION Threats to External Validity. Although we have covered as many types of VR applications (developed by diverse Unity versions) as possible, there are still types of scenarios and interaction patterns not fully covered in our approach. Moreover, while our framework is currently tailored for VR applications developed in Unity, it can be extended to sup- port other engines, such as Unreal Engine [35], with further adaptation, thereby enabling its broader applicability. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 15 in source list: Jian Hu. "CG-FL: A data augmentation approach using context-aware genetic algorithm for fault localization", Journal of Systems and Software, 2025"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.jss.2025.112359', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">Threats to Internal Validity</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. The primary <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 15 in source list: Jian Hu. "CG-FL: A data augmentation approach using context-aware genetic algorithm for fault localization", Journal of Systems and Software, 2025"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.jss.2025.112359', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">threats to internal validity</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> arise from <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 15 in source list: Jian Hu. "CG-FL: A data augmentation approach using context-aware genetic algorithm for fault localization", Journal of Systems and Software, 2025"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.jss.2025.112359', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">potential</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> human errors during <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 15 in source list: Jian Hu. "CG-FL: A data augmentation approach using context-aware genetic algorithm for fault localization", Journal of Systems and Software, 2025"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.jss.2025.112359', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">the</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> selection <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 15 in source list: Jian Hu. "CG-FL: A data augmentation approach using context-aware genetic algorithm for fault localization", Journal of Systems and Software, 2025"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1016/j.jss.2025.112359', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#866712" class="#866712">of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> Core Code and the implementation of the interface. Specifically, during the Core Code selection phase, errors could be introduced due to subjective judgment. To minimize this threat, we have leveraged a majority-voting mechanism, where four test engineers (with VR developing experience) have independently selected code segments. The final decision has been made according to the majority vote, thereby ensuring a more objective outcome. Moreover, during the interface implementation phase, potential inconsistencies or biases among test engineers are also present. To address Coverage (%) 80 81.67% 91.43% 60 66.53% 40 VRExplorer Line Coverage VRExplorer Interactable Coverage 20 VRGuide Line Coverage 0 VRGuide Interactable Coverage 100 Move Speed = 3 m/s, Turn Speed = 30 deg/s 100.00% 80 81.67% 94.29% Coverage (%) 60 66.53% 40 VRExplorer Line Coverage VRExplorer Interactable Coverage 20 VRGuide Line Coverage 0 VRGuide Interactable Coverage 0 25 50 75 100 125 150 175 200 Time (s) 100 Move Speed = 6 m/s, Turn Speed = 30 deg/s 100.00% 0 20 40 60 80 100 120 140 160 Fig. 8: Coverage Performance of unity-vr-maze on different Time (s) 94.29% Coverage (%) 80 81.67% 60 66.53% 40 VRExplorer Line Coverage VRExplorer Interactable Coverage 20 VRGuide Line Coverage 0 VRGuide Interactable Coverage 0 25 50 75 100 125 150 175 Time (s) 100 Move Speed = 6 m/s, Turn Speed = 60 deg/s 100.00% speed parameters (MS and TS) this threat, we have arranged the four test engineers to col- laboratively discuss the design to reach a consensus, thereby helping mitigate discrepancies and ensuring a consistent yet customized implementation. Impacts of Speed Parameters. To explore optimal speed parameters, we test three sets of values on the unity-vr-maze project. Specifically, we have chosen (i) MS = 3 m/s, TS = 30 deg/s, (ii) MS = 6 m/s, TS = 30 deg/s; and (iii) MS = 6 m/s, TS = 60 deg/s. During the experiments, all of the initial position metric is fixed at (0,4.5,0) to ensure the test tool be legally standing on the floor. Fig. 8 shows the coverage performance of VRExplorer with comparison of VRGuide in unity-vr-maze with different speed parameters. We observe the same trends for all three parameters. In particular, VRExplorer takes less time to reach convergence than the baseline approach VRGuide with the higher ELOC coverage and IO coverage. These results also indicate that the TS has the minimal impact on efficiency, whereas the MS shows a more significant effect. Limitations and Future Work. VRExplorer still needs to analyze scenes manually and implement customized interfaces, thereby inevitably introducing some extra workloads (despite low) and a certain learning curve for test engineers. As future work, we will improve VRExplorer’s analysis capability of automatically understanding VR scenes inspired by recent advances in multi-modal large models. Further, we will also aim to automate the interface implementation and task model generation with the advent of large language models. VII. RELATED WORK Automated Game Testing. As a code-based data augmen- tation technique, GLIB [36] can automatically detect game GUI glitches. Macklon et al. [37] present an approach <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 23 in source list: Finlay Macklon, Mohammad Reza Taesiri, Markos Viggiato, Stefan Antoszko, Natalia Romanova, Dale Paas, Cor-Paul Bezemer. "Automatically Detecting Visual Bugs in HTML5 Canvas Games", Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering, 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1145/3551349.3556913', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:blue" class="blue">to automatically detect visual bugs in &lt;canvas&gt; games</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. Prasetya et al. [38] leverage graph-based path-finding techniques to tackle challenges in automated game navigation and explo- ration. As a Java-based multi-agent programming framework, IX4XR [39, 40] facilitates game testing by enabling test agents to interact with the game under test. As a Belief- Desire-Intention library, APLIB [29] supports the development of intelligent agents capable of executing complex testing tasks. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 28 in source list: Wilkins, Ben. "Learning to Identify Bugs in Video Games", 2023"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=17.048496069658&svr=6&lang=zh_hans&sid=1717001602&n=3793&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">Ferdous et al</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. [41] propose <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 28 in source list: Wilkins, Ben. "Learning to Identify Bugs in Video Games", 2023"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=17.048496069658&svr=6&lang=zh_hans&sid=1717001602&n=3793&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">a model-based approach</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> by leveraging EFSM <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 28 in source list: Wilkins, Ben. "Learning to Identify Bugs in Video Games", 2023"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=17.048496069658&svr=6&lang=zh_hans&sid=1717001602&n=3793&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#330099" class="#330099">to</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> model game behavior. However, these game testing approaches are generally tailored to specific types of games instead of VR applications. Automated Mobile Application Testing. Stoat [42] is a <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 31 in source list: Bo Jiang, Wenlin Wei, Li Yi, W.K. Chan. "DroidGamer: Android Game Testing with Operable Widget Recognition by Deep Learning*", 2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/QRS54544.2021.00031', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">stochastic model-based testing</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> tool for <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 31 in source list: Bo Jiang, Wenlin Wei, Li Yi, W.K. Chan. "DroidGamer: Android Game Testing with Operable Widget Recognition by Deep Learning*", 2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/QRS54544.2021.00031', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">Android apps</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> with combined <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 31 in source list: Bo Jiang, Wenlin Wei, Li Yi, W.K. Chan. "DroidGamer: Android Game Testing with Operable Widget Recognition by Deep Learning*", 2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS), 2021"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/QRS54544.2021.00031', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">dynamic</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> and static analysis to generate tests. Chen <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 27 in source list: https://profs.scienze.univr.it/~ceccato/papers/2021/scam2021.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=17.048496069658&svr=6&lang=zh_hans&sid=2988075410&n=3799&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">et al</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. [43] <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 27 in source list: https://profs.scienze.univr.it/~ceccato/papers/2021/scam2021.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=17.048496069658&svr=6&lang=zh_hans&sid=2988075410&n=3799&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">propose a model-based</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> GUI testing <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 27 in source list: https://profs.scienze.univr.it/~ceccato/papers/2021/scam2021.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=17.048496069658&svr=6&lang=zh_hans&sid=2988075410&n=3799&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#0270B6" class="#0270B6">approach for</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> HarmonyOS applications with the adoption of the arkxtest[44] framework. AutoConsis [45] leverages a specially tailored <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 21 in source list: https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2025.1550407/pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=17.048496069658&svr=6&lang=zh_hans&sid=2928666550&n=3811&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#D10A0A" class="#D10A0A">Contrastive Language-Image Pre-training (CLIP) multi-modal model</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> to automatically analyze Android 2D GUI pages. FAST- BOT2 [46] leverages a probabilistic model that memorizes key information for testing based on a model-guided testing strategy. KEA [47] is a property-based testing tool for finding functional bugs in Android apps. However, these mobile ap- plication testing approaches can only address 2D GUI testing. RL-Based Testing. Tufano et al. [48] employ <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: https://arxiv.org/pdf/2201.06865.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=17.048496069658&svr=6&lang=zh_hans&sid=969669765&n=3797&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">RL to train an agent to play</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> games in <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: https://arxiv.org/pdf/2201.06865.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=17.048496069658&svr=6&lang=zh_hans&sid=969669765&n=3797&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">a human</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">-like manner <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 2 in source list: https://arxiv.org/pdf/2201.06865.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=17.048496069658&svr=6&lang=zh_hans&sid=969669765&n=3797&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">to identify areas</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> that lead to FPS drops. RLBT [28] applies a curiosity- based RL approach to automate game testing by maximizing coverage. Bergdahl et al. [49] adopts a modular approach, in which RL complements <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 34 in source list: Joakim Bergdahl, Camilo Gordillo, Konrad Tollmar, Linus Gisslen. "Augmenting Automated Game Testing with Deep Reinforcement Learning", 2020 IEEE Conference on Games (CoG), 2020"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/CoG47356.2020.9231552', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">classical test scripting rather than</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> replacing <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 34 in source list: Joakim Bergdahl, Camilo Gordillo, Konrad Tollmar, Linus Gisslen. "Augmenting Automated Game Testing with Deep Reinforcement Learning", 2020 IEEE Conference on Games (CoG), 2020"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/CoG47356.2020.9231552', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#ce0031" class="#ce0031">it</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. Wuji [50] <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 11 in source list: https://web.archive.org/web/20220823101309if_/https:/arxiv.org/pdf/2208.07811v2.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=17.048496069658&svr=6&lang=zh_hans&sid=1115539198&n=3799&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">leverages evolutionary algorithms, RL, and multi-objective optimization</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> for <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 11 in source list: https://web.archive.org/web/20220823101309if_/https:/arxiv.org/pdf/2208.07811v2.pdf"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=17.048496069658&svr=6&lang=zh_hans&sid=1115539198&n=3799&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#006331" class="#006331">game testing</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. However, RL-based approaches alone are very limited in complex VR scene exploration. VR Application Testing. Rzig et al. [8] analyze <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 22 in source list: Dhia Elhaq Rzig, Nafees Iqbal, Isabella Attisano, Xue Qin, Foyzul Hassan. "Virtual Reality (VR) Automated Testing in the Wild: A Case Study on Unity-Based VR Applications", Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1145/3597926.3598134', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">314 open- source VR applications</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">, revealing <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 22 in source list: Dhia Elhaq Rzig, Nafees Iqbal, Isabella Attisano, Xue Qin, Foyzul Hassan. "Virtual Reality (VR) Automated Testing in the Wild: A Case Study on Unity-Based VR Applications", Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1145/3597926.3598134', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#287B28" class="#287B28">that 79% of</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> them lack automated tests. Harms [51] proposes an automated approach that extracts task trees from real VR usage recordings to detect usability smells without predefined tasks or settings. PREDART [9] predicts human ratings of virtual object place- ments, serving as test oracles in AR testing. VRTest [10] <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: Xiaoyin Wang. "VRTest: An Extensible Framework for Automatic Testing of Virtual Reality Scenes", 2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE-Companion55297.2022.9793753', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">extracts information from a VR scene and controls the user</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">’s <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 9 in source list: Xiaoyin Wang. "VRTest: An Extensible Framework for Automatic Testing of Virtual Reality Scenes", 2022 IEEE/ACM 44th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion), 2022"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ICSE-Companion55297.2022.9793753', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#227967" class="#227967">camera to explore the scene and interact with virtual objects</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. VRGuide [11] applies a computational <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: Xiaoyin Wang, Tahmid Rafi, Na Meng. "VRGuide: Efficient Testing of Virtual Reality Scenes via Dynamic Cut Coverage", 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ASE56229.2023.00197', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">geometry technique called Cut Extension to optimize camera routes for covering all</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> interactable <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 10 in source list: Xiaoyin Wang, Tahmid Rafi, Na Meng. "VRGuide: Efficient Testing of Virtual Reality Scenes via Dynamic Cut Coverage", 2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE), 2023"><a href="javascript:void(0);" onClick="window.open('https://doi.org/10.1109/ASE56229.2023.00197', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#CB0099" class="#CB0099">objects</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match">. Qin and Weaver [12] explore Generative AI for field of view analysis in VR testing. However, these approaches can not address complicated VR interactions and are not generic for different development ecosystems. VIII. CONCLUSION In this paper, we design the EAT framework, a generic three-layer abstraction framework based on OOP for modeling complex interaction behaviors and tasks in testing VR applica- tions. Based on EAT, we present VRExplorer, a novel model- based testing tool to achieve effective interactions with diverse virtual objects and explorations of complex VR scenes. <img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" height="1" width="1" alt="Begin Match to source 6 in source list: https://arxiv.org/html/2402.14096v1"><a href="javascript:void(0);" onClick="window.open('newreport_context.asp?r=17.048496069658&svr=6&lang=zh_hans&sid=61196225&n=3807&svr=6&session-id=fdfc33d37064482f8bbe81ab660e9cf7', 'context', 'location=no,menubar=no,resizable=yes,scrollbars=yes,titlebar=no,toolbar=no,status=no')" style="color:#630000" class="#630000">To validate the performance of our approach, we</a><img src="/r/build/images/new_dynamic/df3e567d6f16d040326c7a0ea29a4f41cb_spacer.gif" alt="End Match"> evaluate VREx- plorer on nine representative VR projects. The experimental results validate our approach’s superior performance to the SOTA method in terms of coverage and efficiency, as well as the ability to detect complicated real-world bugs. REFERENCES [1] R. Radoeva, “Overview on hardware characteristics of virtual reality systems,” in 2022 International Congress on Human-Computer Interaction, Optimization and Robotic Applications (HORA). IEEE, 2022, p. 31.00. [2] P. Rajeswaran, J. Varghese, P. Kumar, J. Vozenilek, and T. Kesavadas, “AirwayVR: Virtual Reality Trainer for Endotracheal Intubation,” in 2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR), 2019, pp. 1345–1346. [3] D. Hamilton, J. McKechnie, E. Edgerton et al., “Immer- sive virtual reality as a pedagogical tool in education: a systematic literature review of quantitative learning out- comes and experimental design,” Journal of Computers in Education, vol. 8, pp. 1–32, 2021. [4] Y. Liu, Q. Sun, Y. Tang, Y. Li, W. Jiang, and J. Wu, “Virtual reality system for industrial training,” in 2020 International Conference on Virtual Reality and Visual- ization (ICVRV), 2020, pp. 338–339. [5] B. Wang, L. Zheng, Y. Wang, W. Fang, and L. Wang, “Towards the industry 5.0 frontier: Review and prospect of XR in product assembly,” Journal of Manufacturing Systems, vol. 74, pp. 777–811, 2024. [6] M. A. Muhanna, “Virtual reality and the cave: Taxonomy, interaction challenges and research directions,” Journal of King Saud University - Computer and Information Sciences, vol. 27, no. 3, pp. 344–361, 2015. [7] F. B. Insights, “Virtual Reality (VR) Market Size, Share & Industry Analysis, By Component (Hardware, Software, and Content), By Device Type (Head Mounted Display (HMD), VR Simulator, VR Glasses, Treadmills & Haptic Gloves, and Others), By Industry (Gaming, Entertainment, Automotive, Retail, Healthcare, Education, Aerospace & Defense, Manufacturing, and Others), and Regional Forecast, 2024-2032,” https://www.fortunebusinessinsights.com/ industry-reports/virtual-reality-market-101378, 2024. [8] D. E. Rzig, N. Iqbal, I. Attisano, X. Qin, and F. Hassan, “Virtual Reality (VR) Automated Testing in the Wild: A Case Study on Unity-Based VR Applications,” in Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis, 2023, p. 1269–1281. [9] T. Rafi, X. Zhang, and X. Wang, “Predart: Towards automatic oracle prediction of object placements in augmented reality testing,” in Proceedings of the 37th IEEE/ACM International Conference on Automated Soft- ware Engineering, ser. ASE ’22, 2023. [10] X. Wang, “VRTest: An Extensible Framework for Au- tomatic Testing of Virtual Reality Scenes,” in 2022 IEEE/ACM 44th International Conference on Soft- ware Engineering: Companion Proceedings (ICSE- Companion), 2022, pp. 232–236. [11] X. Wang, T. Rafi, and N. Meng, “Vrguide: Efficient testing of virtual reality scenes via dynamic cut cover- age,” in Proceedings of the 38th IEEE/ACM International Conference on Automated Software Engineering, ser. ASE ’23. IEEE Press, 2024, p. 951–962. [12] X. Qin and G. Weaver, “Utilizing Generative AI for VR Exploration Testing: A Case Study,” in Proceedings of the 39th IEEE/ACM International Conference on Auto- mated Software Engineering Workshops, ser. ASEW ’24, 2024, p. 228–232. [13] S. Li, B. Li, Y. Liu, C. Gao, J. Zhang, S.-C. Cheung, and M. R. Lyu, “Grounded GUI Understanding for Vision Based Spatial Intelligent Agent: Exemplified by Virtual Reality Apps,” 2024. [Online]. Available: https://arxiv.org/abs/2409.10811 [14] VentureBeat, “Unity financial results (q2-2024),” 2024. [Online]. Available: https://venturebeat.com/ games/unity-financial-results-q2-2024/ [15] “XR Interaction Toolkit,” 2024. [Online]. Avail- able: https://docs.unity3d.com/Packages/com.unity.xr. interaction.toolkit@3.0/manual/index.html [16] V. Juránek, “Virtual reality toolkit for the unity game engine [online],” 2021 [cit. 2025-01-24]. [17] “SteamVR,” 2024. [Online]. Available: https://github. com/ValveSoftware/steamvr unity plugin [18] S. Bovet, A. Kehoe, K. Crowley, N. Curran, M. Gutierrez, M. Meisser, D. O. Sullivan, and T. Rouvinez, “Using Traditional Keyboards in VR: SteamVR Developer Kit and Pilot Game User Study,” in 2018 IEEE Games, Entertainment, Media Conference (GEM), 2018, pp. 1–9. [19] V. Corporation, “Steam VR Plugin,” https: //assetstore.unity.com/packages/tools/integration/ steamvr-plugin-32647, 2024. [20] “Mixed reality toolkit,” 2022. [Online]. Available: https://github.com/microsoft/MixedRealityToolkit-Unity [21] S. Ong and V. K. Siddaraju, Introduction to the Mixed Reality Toolkit. Berkeley, CA: Apress, 2021, pp. 85–110. [Online]. Available: https://doi.org/10.1007/ 978-1-4842-7104-9 4 [22] “Unity monobehaviour class,” 2022. [Online]. Avail- able: https://docs.unity3d.com/2022.3/Documentation/ ScriptReference/MonoBehaviour.html [23] U. Technologies, “Building a navigation mesh,” 2020. [Online]. Available: https://docs.unity3d.com/2020.1/ Documentation/Manual/nav-BuildingNavMesh.html [24] Unity Technologies, “Unity - manual: Navmesh agent,” 2025. [Online]. Available: https://docs.unity3d.com/ Manual/class-NavMeshAgent.html [25] U. Technologies, “Unity - manual: Nav mesh obstacle,” Unity Technologies, 2025. [Online]. Available: https:// docs.unity3d.com/Manual/class-NavMeshObstacle.html [26] Unity Technologies, “UnityEvent Documentation,” 2025. [Online]. Available: https://docs.unity3d.com/6000.0/ Documentation/ScriptReference/Events.UnityEvent.html [27] Y. Lan, Y. Lu, M. Pan, and X. Li, “Navigating Mobile Testing Evaluation: A Comprehensive Statistical Analy- sis of Android GUI Testing Metrics,” in Proceedings of the 39th IEEE/ACM International Conference on Auto- mated Software Engineering, 2024, p. 944–956. [28] R. Ferdous, F. Kifetew, D. Prandi, and A. Susi, “Towards Agent-Based Testing of 3D Games using Reinforcement Learning,” in the 37th IEEE/ACM International Confer- ence on Automated Software Engineering, 2023. [29] I. S. W. B. Prasetya, M. Dastani, R. Prada, T. E. J. Vos, F. Dignum, and F. Kifetew, “Aplib: Tactical Agents for Testing Computer Games,” in Engineering Multi-Agent Systems. Springer, 2020, pp. 21–41. [30] “Code coverage,” 2023. [Online]. Available: https://github.com/needle-mirror/com.unity.testtools. codecoverage [31] M. Foxman, B. Klebig, A. Leith, D. Beyea, R. Ratan, and V. Chen, “Virtual reality genres: Comparing prefer- ences in immersive experiences and games,” in Extended Abstracts of the 2020 Annual Symposium on Computer- Human Interaction in Play, 11 2020. [32] MITRE, “Cwe-395: Use of nullpointerexception catch to detect null pointer dereference,” https://cwe.mitre.org/ data/definitions/395.html, 2021. [33] GitHub CodeQL, “Avoid catching nullreferenceex- ception,” https://codeql.github.com/codeql-query-help/ csharp/cs-catch-nullreferenceexception/, 2022. [34] Unity Technologies, “Unity Scripting API: Serialize- Field,” https://docs.unity3d.com/6000.1/Documentation/ ScriptReference/SerializeField.html, 2024. [35] Epic Games, “Unreal engine 5,” 2025. [On- line]. Available: https://www.unrealengine.com/en-US/ unreal-engine-5 [36] K. Chen, Y. Li, Y. Chen, C. Fan, Z. Hu, and W. Yang, “Glib: towards automated test oracle for graphically- rich applications,” in Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engi- neering, ser. ESEC/FSE 2021, 2021, p. 1093–1104. [37] F. Macklon et al., “Automatically Detecting Visual Bugs in HTML5 Canvas Games,” in Proceedings of the 37th IEEE/ACM International Conference on Automated Soft- ware Engineering, ser. ASE ’22, 2023. [38] I. S. W. B. Prasetya et al., “Navigation and exploration in 3D-game automated play testing,” in Proceedings of the 11th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation, ser. ESEC/FSE ’20. ACM, Nov. 2020, p. 3–9. [Online]. Available: http://dx.doi.org/10.1145/ 3412452.3423570 [39] I. S. W. B. Prasetya, F. Pastor Ricós, F. M. Kifetew, D. Prandi, S. Shirzadehhajimahmood, T. E. J. Vos, P. Paska, K. Hovorka, R. Ferdous, A. Susi, and J. David- son, “An agent-based approach to automated game test- ing: an experience report,” in Proceedings of the 13th International Workshop on Automating Test Case Design, Selection and Evaluation, ser. A-TEST ’22. ACM, Nov. 2022. [40] S. Shirzadehhajimahmood, I. S. W. B. Prasetya, F. Dignum, M. Dastani, and G. Keller, “Using an agent- based approach for robust automated testing of computer games,” in Proceedings of the 12th International Work- shop on Automating TEST Case Design, Selection, and Evaluation, ser. A-TEST 2021, 2021, p. 1–8. [41] R. Ferdous, F. Kifetew, D. Prandi, I. S. W. B. Prasetya, S. Shirzadehhajimahmood, and A. Susi, “Search-based automated play testing of computer games: A model- based approach,” in Search-Based Software Engineering: 13th International Symposium, SSBSE 2021, Bari, Italy, October 11–12, 2021, Proceedings, 2021, p. 56–71. [42] T. Su, G. Meng, Y. Chen, K. Wu, W. Yang, Y. Yao, G. Pu, Y. Liu, and Z. Su, “Guided, stochastic model-based GUI testing of Android apps,” in Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering, ser. ESEC/FSE 2017, 2017, p. 245–256. [43] Y. Chen, S. Wang, Y. Tao, and Y. Liu, “Model-based GUI Testing For HarmonyOS Apps,” in Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering, ser. ASE ’24, 2024, p. 2411–2414. [44] “Arkxtest,” 2022. [Online]. Available: https://gitee.com/ openharmony/testfwk arkxtest [45] Y. Hu, H. Jin, X. Wang, J. Gu, S. Guo, C. Chen, X. Wang, and Y. Zhou, “AutoConsis: Automatic GUI- driven Data Inconsistency Detection of Mobile Apps,” in Proceedings of the 46th International Conference on Software Engineering: Software Engineering in Practice, ser. ICSE-SEIP ’24, 2024, p. 137–146. [46] Z. Lv, C. Peng, Z. Zhang, T. Su, K. Liu, and P. Yang, “Fastbot2: Reusable Automated Model-based GUI Test- ing for Android Enhanced by Reinforcement Learning,” in Proceedings of the 37th IEEE/ACM International Con- ference on Automated Software Engineering, ser. ASE ’22, 2023. [47] Y. Xiong, T. Su, J. Wang, J. Sun, G. Pu, and Z. Su, “General and practical property-based testing for android apps,” in Proceedings of the 39th IEEE/ACM Interna- tional Conference on Automated Software Engineering, ser. ASE ’24, 2024, p. 53–64. [48] R. Tufano, S. Scalabrino, L. Pascarella, E. Aghajani, R. Oliveto, and G. Bavota, “Using reinforcement learning for load testing of video games,” in 2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE), 2022, pp. 2303–2314. [49] J. Bergdahl, C. Gordillo, K. Tollmar, and L. Gisslén, “Augmenting automated game testing with deep rein- forcement learning,” 2020 IEEE Conference on Games (CoG), pp. 600–603, 2020. [50] Y. Zheng, X. Xie, T. Su, L. Ma, J. Hao, Z. Meng, Y. Liu, R. Shen, Y. Chen, and C. Fan, “Wuji: Automatic online combat game testing using evolutionary deep reinforce- ment learning,” in 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE), 2019, pp. 772–784. [51] P. Harms, “Automated usability evaluation of virtual reality applications,” ACM Transactions on Computer- Human Interaction, vol. 26, no. 3, Apr. 2019. </p></div></body></html>
