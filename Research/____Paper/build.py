import os
import json
import urllib.parse
import re
from datetime import datetime
from pathlib import Path

PDF_DIR = "papers"
METADATA_FILE = "metadata.json"
OUTPUT_HTML = "index.html"
TEMPLATE_HTML = "template.html"

os.makedirs(PDF_DIR, exist_ok=True)

# æ ¹æ®æ–‡ä»¶åè‡ªåŠ¨æ¨æ–­ä¼šè®®/å¹´ä»½ï¼ˆå¦‚æœåŒ¹é…ä¸åˆ°åˆ™è¿”å› Noneï¼‰
def infer_venue_and_year(fname):
    fname_lower = fname.lower()
    patterns = [
        (r"ase(?:20)?(\d{2})", "ASE"),
        (r"icse(?:20)?(\d{2})", "ICSE"),
        (r"fse(?:20)?(\d{2})", "FSE"),
        (r"sbst(?:20)?(\d{2})", "SBST"),
        (r"issta(?:20)?(\d{2})", "ISSTA"),
        (r"acl(?:20)?(\d{2})", "ACL"),
        (r"arxiv(?:20)?(\d{2})", "arXiv"),
        (r"nip(?:s)?(?:20)?(\d{2})", "NeurIPS"),
        (r"cvpr(?:20)?(\d{2})", "CVPR"),
        (r"ismar(?:20)?(\d{2})", "ISMAR"),
        (r"iccv(?:20)?(\d{2})", "ICCV"),
        (r"eccv(?:20)?(\d{2})", "ECCV"),
        (r"aaai(?:20)?(\d{2})", "AAAI"),
        (r"icra(?:20)?(\d{2})", "ICRA"),
        (r"uist(?:20)?(\d{2})", "UIST"),
        (r"sec(?:20)?(\d{2})", "Usenix SEC"),
        (r"iva(?:20)?(\d{2})", "IVA"),
        (r"chi(?:20)?(\d{2})", "CHI"),
        (r"siggraph(?:20)?(\d{2})", "SIGGRAPH"),
    ]
    for pat, venue_name in patterns:
        match = re.search(pat, fname_lower)
        if match:
            year_suffix = match.group(1)
            year = f"20{year_suffix}"
            return venue_name, year
    return None, None

# åŠ è½½æˆ–åˆ›å»º metadata
if os.path.exists(METADATA_FILE):
    with open(METADATA_FILE, "r", encoding="utf-8") as f:
        metadata = json.load(f)
else:
    metadata = {}

# æ‰«æ PDF æ–‡ä»¶
papers = []
pdf_files = []
for root, _, files in os.walk(PDF_DIR):
    for fname in files:
        if fname.lower().endswith(".pdf"):
            abs_path = os.path.join(root, fname)
            rel_path = os.path.relpath(abs_path, PDF_DIR)
            rel_path = rel_path.replace(os.sep, "/")
            pdf_files.append((rel_path, fname))

pdf_files.sort(key=lambda item: item[0].lower())
legacy_keys_to_remove = set()

for rel_path, fname in pdf_files:
    key = rel_path.lower()
    legacy_key = fname.lower()

    info = metadata.get(key) or metadata.get(legacy_key, {})
    if legacy_key in metadata and key != legacy_key:
        legacy_keys_to_remove.add(legacy_key)

    # URL ç¼–ç è·¯å¾„
    quoted_rel_path = "/".join(urllib.parse.quote(part) for part in rel_path.split("/"))

    # è‡ªåŠ¨ç”Ÿæˆ tagï¼šå¦‚æœ metadata ä¸­æ²¡æœ‰ tags æˆ–ä¸ºç©ºï¼Œåˆ™ä½¿ç”¨ PDF æ‰€åœ¨çš„ä¸€çº§æ–‡ä»¶å¤¹
    folder_tag = rel_path.split("/")[0] if "/" in rel_path else "unsorted"
    tags = info.get("tags")
    if not tags:
        tags = [folder_tag]

    # è‡ªåŠ¨ä»æ–‡ä»¶åæ¨æ–­å¹´ä»½å’Œä¼šè®®
    year = info.get("year")
    venue = info.get("venue")
    if not year or not venue:
        inferred_venue, inferred_year = infer_venue_and_year(fname)
        if not year:
            year = inferred_year  # åªæœ‰åŒ¹é…åˆ°æ‰å¡«
        if not venue:
            venue = inferred_venue  # åªæœ‰åŒ¹é…åˆ°æ‰å¡«

    paper = {
        "file_key": key,
        "title": info.get("title", os.path.splitext(fname)[0]),
        "authors": info.get("authors", "Unknown"),
        "year": year if year else "",      
        "venue": venue if venue else "",   
        "tags": tags,
        "pdf": f"{PDF_DIR}/{quoted_rel_path}",
        "pdf_local": f"{PDF_DIR}/{quoted_rel_path}",
        "read": info.get("read", False),
        "bib": info.get("bib", ""),
        "notes": info.get("notes", "")
    }

    papers.append(paper)
    metadata[key] = paper

# åˆ é™¤ legacy key
for legacy_key in legacy_keys_to_remove:
    metadata.pop(legacy_key, None)

# âœ… æ¸…ç† metadata ä¸­å·²ä¸å­˜åœ¨çš„ PDF
existing_keys = set([rel_path.lower() for rel_path, _ in pdf_files])
keys_to_remove = [k for k in metadata if k not in existing_keys]
for k in keys_to_remove:
    print(f"ğŸ—‘ï¸ åˆ é™¤å·²ä¸å­˜åœ¨çš„ PDF å¯¹åº” metadata: {k}")
    metadata.pop(k, None)

# ä¿å­˜ metadata.json
with open(METADATA_FILE, "w", encoding="utf-8") as f:
    json.dump(metadata, f, indent=2, ensure_ascii=False)

# æ¸²æŸ“ HTML
if not os.path.exists(TEMPLATE_HTML):
    raise FileNotFoundError(f"{TEMPLATE_HTML} ä¸å­˜åœ¨")

with open(TEMPLATE_HTML, "r", encoding="utf-8") as f:
    html = f.read()

html = html.replace("const EMBEDDED_PAPERS = [];", f"const EMBEDDED_PAPERS = {json.dumps(papers, ensure_ascii=False)};")

with open(OUTPUT_HTML, "w", encoding="utf-8") as f:
    f.write(html)

print(f"âœ… æˆåŠŸç”Ÿæˆ {OUTPUT_HTML}ï¼Œå…± {len(papers)} ç¯‡è®ºæ–‡")
